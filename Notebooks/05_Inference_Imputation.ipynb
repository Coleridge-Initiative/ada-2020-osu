{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img style=\"float: center;\" src=\"images/CI_horizontal.png\" width=\"600\"></center>\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Julia Lane, Benjamin Feder, Tian Lou, Lina Osorio-Copete </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcome measurement and imputation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "What should you do when you encounter missing values in your data? Unfortunately, there is usually no *right* answer. However, you can try to impute these missing values, providing your best guess for each missing point's true value. Here, you will learn how to implement common imputation methods you can use in approaching missing values in your own work.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "* Gain understanding of the concept of measurement error in the context of a cohort's earnings\n",
    "\n",
    "* Explore options for imputing missing values\n",
    "\n",
    "* Visualize estimate changes following imputation\n",
    "\n",
    "In this notebook, you will focus on 2012-13 Ohio community college graduates' earnings during their first year after graduation, particularly in their first and fourth quarters after graduation. Recall that in the [Data Exploration](01_2_Dataset_Exploration.ipynb) notebook, you examined the earnings distribution for all members of this cohort who had positive earnings in this time period in Ohio. To evaluate the earnings outcomes of all 2012-13 Ohio community college graduates, you need to decide what to do when you cannot find their earnings in the Ohio Unemployment Insurance (UI) wage records. A person may not appear in Ohio's UI wage records for several reasons:\n",
    "- The person is unemployed. \n",
    "- The person is out of labor force, e.g., schooling, childcare, etc...\n",
    "- The person was employed outside of Ohio.\n",
    "- The person's job is not covered in UI wage records, e.g.,self-employed, independent contractors, federal government works, etc. <a href='https://www.nap.edu/read/10206/chapter/11#294'>(Hotz and Scholz, 2002)</a>\n",
    "\n",
    "You will explore the resulting earnings outcomes after applying different earnings imputation methods. The methods covered in this notebook include:\n",
    "- Dropping all \"missing\" values\n",
    "- Filling in zero for people who do not have records in Ohio UI wage records data \n",
    "- Substituting missing values with the average earnings of people who are in the same degree fields and have the same gender\n",
    "- Regression imputation\n",
    "- Adding in Indiana, Missouri, and Illinois UI wage records for the cohort in question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup and Database Connection\n",
    "\n",
    "Before you begin, you neded to run the code cells below to import the libraries and connect to our PostgreSQL database. You should already be familiar with the `matplotlib`, `pandas`, and `numpy` libraries from previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas-related imports\n",
    "import pandas as pd\n",
    "\n",
    "# Numpy\n",
    "import numpy as np\n",
    "\n",
    "# database interaction imports\n",
    "import sqlalchemy\n",
    "\n",
    "#Matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# regression modeling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Jupyter-specific \"magic command\" to plot images directly in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to create a connection to the database, \n",
    "# we need to pass the name of the database and host of the database\n",
    "\n",
    "host = 'stuffed.adrf.info'\n",
    "DB = 'appliedda'\n",
    "\n",
    "connection_string = \"postgresql://{}/{}\".format(host, DB)\n",
    "conn = sqlalchemy.create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create some temporary tables to recall our cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to use degree year (degcert_yr_earned) and degree term (degcert_term_earned) to identify \n",
    "# 2012-13 academic year.\n",
    "# We use campus type to identify community college students \n",
    "# TC=technical college, SC=state college, CC=community college\n",
    "# Drop transfer students (Those with degcert_subject = 'TRAMOD')\n",
    "\n",
    "qry = '''\n",
    "create temp table cc_grads as\n",
    "select *\n",
    "from data_ohio_olda_2018.oh_hei_long a\n",
    "left join data_ohio_olda_2018.oh_hei_campus_county_lkp lkp\n",
    "on a.degcert_campus = lkp.campus_num\n",
    "where ((a.degcert_yr_earned = '2012' and (a.degcert_term_earned = '4' or a.degcert_term_earned = '1')) or \n",
    "    (a.degcert_yr_earned = '2013' and (a.degcert_term_earned = '2' or a.degcert_term_earned = '3'))) and \n",
    "    lkp.campus_type_code in ('TC', 'SC', 'CC') and\n",
    "    a.degcert_subject != 'TRAMOD'\n",
    "'''\n",
    "conn.execute(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most recent graduation within the span of 2012-13 academic year\n",
    "# We convert degree term and degree year to dates and sort each person's records descendingly by date.\n",
    "# Then we only keep each graduate's first record.\n",
    "\n",
    "qry = '''\n",
    "create temp table cc_grads_recent as\n",
    "select distinct on (ssn_hash) *\n",
    "from (\n",
    "SELECT *, \n",
    "    CASE WHEN degcert_term_earned = 4 THEN\n",
    "        format('%%s-%%s-01', degcert_yr_earned, 7)::date \n",
    "    WHEN degcert_term_earned = 1 THEN\n",
    "        format('%%s-%%s-01', degcert_yr_earned, 10)::date \n",
    "    WHEN degcert_term_earned = 2 THEN\n",
    "        format('%%s-%%s-01', degcert_yr_earned, 1)::date \n",
    "    WHEN degcert_term_earned = 3 THEN\n",
    "        format('%%s-%%s-01', degcert_yr_earned, 4)::date \n",
    "    END AS deg_date\n",
    "    from cc_grads\n",
    ") q\n",
    "order by ssn_hash, deg_date DESC\n",
    "'''\n",
    "conn.execute(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Cohort to Ohio UI wage records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we've already joined our cohort of 2012-13 academic year Ohio community college graduates to the Ohio UI wage records data in the table `cohort_oh_jobs` in the `ada_20_osu` schema. For the purposes of this notebook, we slighly adapted `cohort_oh_jobs` to include the graduate's degree (`degcert_subject`) so we can use degree for one of our imputation techniques. The following SQL code was used to recreate `cohort_oh_jobs`, which has already been done for you.\n",
    "\n",
    "    create table ada_20_osu.cohort_oh_jobs as\n",
    "    select a.ssn_hash, a.deg_date, a.degcert_subject, b.job_date, b.sumwages, (b.job_date - a.deg_date) time_after_grad\n",
    "    from cc_grads_recent a\n",
    "    join ada_20_osu.small_ohio_ui b\n",
    "    on a.ssn_hash = b.ssn_hash\n",
    "    where b.job_date > a.deg_date AND (a.deg_date + '1 year'::interval) >= b.job_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at cohort_oh_jobs with subject\n",
    "qry = '''\n",
    "select *\n",
    "from ada_20_osu.cohort_oh_jobs\n",
    "limit 5;\n",
    "'''\n",
    "pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Exploration: Earnings during first quarter after graduation\n",
    "\n",
    "Before we start performing imputation, we need to do some quick data manipulation to isolate earnings from the first quarter after each individual's graduation. To do so, we can create a new column, `qrt_after_grad`, in python by dividing `time_after_grad` by 90 and rounding the nearest whole number. From there, we can see how many members of our cohort had positive earnings in Ohio in their first quarter after graduation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in ada_20_osu.cohort_oh_jobs in the dataframe `df`\n",
    "qry = '''\n",
    "select *\n",
    "from ada_20_osu.cohort_oh_jobs\n",
    "'''\n",
    "df = pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert days after graduation into quarters\n",
    "df['qrt_after_grad'] = round(df.loc[:,('time_after_grad')]/90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Since the `time_after_grad` column is generated by subtracting the first day of different quarters divided by 90 it will capture all first quarter after grad records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter quarter 1 after graduation\n",
    "df_q1 = df[df['qrt_after_grad']==1]\n",
    "df_q1 = df_q1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q1['ssn_hash'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total graduates with positive earnings during first quarter after graduation: {:,.0f}'\\\n",
    ".format(df_q1['ssn_hash'].nunique()))\n",
    "print('That is {:.1f}% of the study cohort'\\\n",
    ".format((df_q1['ssn_hash'].nunique()/df['ssn_hash'].nunique())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red\">Checkpoint 1: Identifying Earnings in the Fourth Quarter after Graduation</h3>\n",
    "\n",
    "Given the code above, create a data subset `df_q4` that contains all earnings for our cohort in their fourth quarter after graduation. How many members of our cohort had positive earnings in this quarter? Do you expect this number to be higher or lower than the number in the first quarter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint: you can refer to the code above for filtering `df` when 'qrt_after_grad' is 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add graduates without positive earnings for Q1\n",
    "\n",
    "If we were to subset `cohort_oh_jobs` to their first quarter after graduation (`df_q1`), this table would only contain individuals with positive earnings in their first quarter after graduation in Ohio. Let's add in members of our cohort who did not appear in Ohio's wage records during this time period into the group in `cohort_oh_jobs`. This will let us easily analyze different earnings distributions in our cohort's first quarter after graduation moving forward.\n",
    "\n",
    "> Before we do this, we will grab some demographic information on our cohort for future imputation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find demographics information for all graduates in our cohort\n",
    "qry = '''\n",
    "select distinct a.ssn_hash, a.deg_date, a.degcert_subject, b.birthdate_y, b.race_ethnic_code, b.gender\n",
    "from cc_grads_recent a\n",
    "LEFT JOIN data_ohio_olda_2018.oh_hei_demo b \n",
    "on a.ssn_hash = b.ssn_hash\n",
    "'''\n",
    "grads = pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see grads\n",
    "grads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see total number of members in our cohort\n",
    "grads['ssn_hash'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see number of unique ssn_hash values in our cohort\n",
    "grads['ssn_hash'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have differing values when running `.count()` and `.nunique`, it's clear that we have a duplicate `ssn_hash` in `grads`. Let's see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the duplicate\n",
    "grads[grads['ssn_hash'].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is likely that due to clerical errors in the data collection the difference is a typo. Given the information on the other variables, we can assume that both rows correspond to the same `ssn_hash`. We will replace this discrepancy as `null` and then drop the duplicate row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing with NULL\n",
    "grads.loc[(grads['ssn_hash'].duplicated(keep=False)), 'birthdate_y'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "grads = grads.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that number of ssn_hash values is same as nunique()\n",
    "grads['ssn_hash'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check to see if we have any missing values for our demographic variables. If so, let's fill these in as `unknown` so they won't be dropped in future analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see null counts for demographic variables\n",
    "grads.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Theoretically, you could apply these imputation methods to these missing demographic values. However, for the purposes of this notebook, we will focus our imputation techniques on missing earnings values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace null values on grads for 'unknown'\n",
    "grads.fillna('unknown', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have confirmed that `grads` looks as intended, we can merge it with earnings outcomes for our cohort's first quarter after graduation. In the following left join, if a member of our cohort did not appear in the Ohio UI wage records, they will have `NULL` earnings. Below, `merge` works very similarly to SQL's `JOIN` in that you designate the two tables to merge and then describe how you will merge the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left join earnings information for Q1 after graduation on grads\n",
    "cohort_oh_jobs_q1 = pd.merge(grads, df_q1[['ssn_hash', 'sumwages']], \n",
    "                             how = 'left', on='ssn_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see cohort_oh_jobs_q1\n",
    "cohort_oh_jobs_q1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how many have missing earnings values\n",
    "cohort_oh_jobs_q1['sumwages'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, you could make sure that the number of unique `ssn_hash` values from `grads` is equal to the sum of the number of unique `ssn_hash` values from `df_q1` and the number of rows with null `sumwages` in `cohort_oh_jobs_q1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "cohort_oh_jobs_q1['sumwages'].isnull().sum() + df_q1['ssn_hash'].nunique()  == grads['ssn_hash'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red\">Checkpoint 2: Replicate for Q4</h3>\n",
    "\n",
    "Create a DataFrame `cohort_oh_jobs_q4` that mirrors `cohort_oh_jobs_q1` except for Q4. Feel free to add in as many code cells as you deem necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute Wage Values\n",
    "\n",
    "Now that we have confirmed that our `cohort_oh_jobs_q1` DataFrame is ready to use for testing our imputation methods, we can get started. To recall, here are the four methods we will be trying out in this notebook:\n",
    "- Dropping all \"missing\" values\n",
    "- Filling in zero for people who do not have records in Ohio UI data\n",
    "- Filling in missing values with the average earnings of people who are in the same degree fields and have the same gender\n",
    "- Regression\n",
    "- Filling in missing values by adding in Indiana, Missouri, and Illinois UI records for the cohort in question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Drop All Missing Values\n",
    "\n",
    "First, let's look at the earnings outcomes during first quarter after graduation when we drop all missing earnings values. Here, by ignoring potentially non-missing values, we are hoping that they mirror the same distribution as the present one. Although this is fairly common, you should **never, ever, ever** use this method in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing values\n",
    "wages_no_missing = cohort_oh_jobs_q1[[\"ssn_hash\",\"sumwages\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see earnings distribution\n",
    "wages_no_missing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:red\">Checkpoint 3: Replicate for Q4</h4>\n",
    "\n",
    "What does the earnings distribution look like for Q4 when you drop missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fill in Missing Values with Zero\n",
    "\n",
    "Next, let's see how the earnings distribution shifts when we encode all missing earnings outcomes as 0. Here, we are assuming that all missing earnings are due to unemployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill all null sumwages with 0\n",
    "wages_zero = cohort_oh_jobs_q1[['ssn_hash','sumwages']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the distribution. How does it vary from the distribution you get in method 1?\n",
    "wages_zero.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing values distribution for comparison\n",
    "wages_no_missing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average earnings if missing are dropped is ${:,.2f}'.format(wages_no_missing['sumwages'].mean()))\n",
    "print('Average earnings if missing are imputed as 0 is ${:,.2f}'.format(wages_zero.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:red\">Checkpoint 4: Replicate for Q4</h4>\n",
    "\n",
    "What does the earnings distribution look like for Q4 when you fill missing values with zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Fill in Missing Values with Major/Gender Mean Earnings\n",
    "\n",
    "Now, instead of either ignoring missing values or assuming the earnings are 0, we will try imputing missing earnings for each individual as the average yearly earnings of the other individuals in our cohort of the same gender and that graduated with the same subject degree (2 digits)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, our strategy is as follows:\n",
    "- Find mean earnings for each subject by gender\n",
    "- Merge the mean earnings for each subject to year's earnings for each member of the cohort by subject\n",
    "- If the earnings are null, replace with mean earnings of subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that adds 0 before subject code in cases in which subject code has 5 digits\n",
    "def add_0(x):\n",
    "    index = 6\n",
    "    if len(x) < index:\n",
    "       return '0' + x\n",
    "    elif len(x) == index:\n",
    "       return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have recently added in a new column into the `oh_hei_long` table that contains subject codes with leading zeros so you won't have to deal with subject codes of length 5. The column is `degcert_subject_upd`. The same applies for `oh_otc`, where the updated column is `hei_subject_code_upd`. These changes are also reflected in the data dictionary located on the class website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying function to add 0 on some subjects that because where declared as numeric ended up with only 5 digits\n",
    "cohort_oh_jobs_q1['degcert_subject2'] = cohort_oh_jobs_q1.loc[:,('degcert_subject')].apply(lambda x: add_0(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's grab the first two digits of every `degcert_subject` to get two-digit subject codes. After that, we can follow our strategy stated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New column with 2 digit subject\n",
    "cohort_oh_jobs_q1['subj_2dig'] = cohort_oh_jobs_q1['degcert_subject2'].str[0 : 2 : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see new column\n",
    "cohort_oh_jobs_q1['subj_2dig'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore all missing earnings\n",
    "subset = cohort_oh_jobs_q1[['subj_2dig', 'gender', 'sumwages']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see subset\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find mean earnings by gender, subject combination\n",
    "sub_gend_w = subset.groupby(['subj_2dig', 'gender'])['sumwages'].agg('mean').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see sub_gend_w\n",
    "sub_gend_w.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will merge the two DataFrames, `sub_gend_w` and `cohort_oh_jobs_q1` using `merge()`.\n",
    "> Note: We will rename the `sumwages` column in `sub_gend_w` so we don't get confused between the mean earnings by degree and earnings for the individual after the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns for merge\n",
    "sub_gend_w.columns = ('subj_2dig', 'gender', 'mean_w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see renamed columns\n",
    "sub_gend_w.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column of mean earnings by major\n",
    "wages_missing_as_mean = pd.merge(cohort_oh_jobs_q1, sub_gend_w, how = 'left', on=['gender', 'subj_2dig'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see wages_missing_as_mean\n",
    "wages_missing_as_mean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can add a new column to `wages_missing_as_mean` to include the mean degree wage if the individual did not appear in the Ohio UI wage records data. To do so, we will use `pandas` `mask()` command, which replaces values if they match a specific condition.\n",
    "> Note: Here, that specific condition is when `sumwages` is `NULL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing missing sumwages by mean of earning by major when possible\n",
    "wages_missing_as_mean['imputed_wages'] = wages_missing_as_mean['sumwages'].mask(\n",
    "    wages_missing_as_mean['sumwages'].isnull(), wages_missing_as_mean['mean_w'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see updated wages_missing_as_mean\n",
    "wages_missing_as_mean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In using this method, there is a chance we could not impute missing values for all individuals in our cohort. If `imputed_wages` is still `NULL`, we can assume there were no individuals in the cohort with non-missing earnings with the same degree/gender combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if any still don't have imputed earnings\n",
    "sum(wages_missing_as_mean['imputed_wages'].isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, it seems as though we do not have available earnings for every combination of gender and 2-digit subject code. For the sake of the exercise, we will ignore the earnings of those whose we could not impute using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wages_missing_as_mean['imputed_wages'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:red\">Checkpoint 5: Replicate for Q4</h4>\n",
    "Impute missing earnings values as the mean earnings of individuals in the cohort with the same gender and subject degree fields. What does the earning distribution look like? For how many individuals could you not impute values using this method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Regression imputation\n",
    "\n",
    "We can also use regression to try to get more accurate earnings values. We build a regression equation from the obervations for which we know the earnings, then use the equation to essentially predict the earnings missing values. This is, in effect, an extension of the mean imputation by subgroup. Here, we will use demographic information of graduates such as birthdate, gender, ethnicity, degree quarter, and major."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the variables that we need for the regression\n",
    "cohort_q1 = cohort_oh_jobs_q1.loc[:,('ssn_hash','sumwages', 'birthdate_y', 'race_ethnic_code', 'deg_date', 'subj_2dig')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unknown birthdate_y values\n",
    "cohort_q1 = cohort_q1[cohort_q1['birthdate_y']!='unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace missing values with -1 because this value doesn't otherwise exist in the earnings table\n",
    "cohort_q1['sumwages'] = cohort_q1['sumwages'].fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use the `get_dummies` function in order to properly treat the categorical variables in our DataFrame. The function will convert all categorical variables to dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make categorical variables dummies\n",
    "df_dummied = pd.get_dummies(cohort_q1[['race_ethnic_code', 'deg_date', 'subj_2dig']].astype(\n",
    "    'category', copy=False), drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in non-categorical variables\n",
    "df_dummied['ssn_hash'] = cohort_q1['ssn_hash']\n",
    "df_dummied['sumwages'] = cohort_q1['sumwages']\n",
    "df_dummied['birthdate_y'] = cohort_q1['birthdate_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummied.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed missing values first\n",
    "df_nona = df_dummied[df_dummied['sumwages'] != -1]\n",
    "# Drop ssn_hash\n",
    "df_reg = df_nona.loc[:, df_nona.columns != ('ssn_hash')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saved missing values for imputation later\n",
    "df_miss = df_dummied[df_dummied['sumwages'] == -1]\n",
    "# drop ssn_hash and empty column sumwages\n",
    "df_pred = df_miss.drop(['ssn_hash', 'sumwages'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see size of df_nona\n",
    "df_nona.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see size of df_miss\n",
    "df_miss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see df_pred\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model creation process for a linear regression can be done using `scikit-learn`. The process is as follows: We will create the model object, then give it the data, and then use the model object to generate our predictions. The model object essentially contains all of the instructions on how to fit the model, and when we give it the data, it fits the model to that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object\n",
    "ols = LinearRegression()\n",
    "\n",
    "# Predictors and Outcome\n",
    "predictors = df_reg.drop(['sumwages'], axis = 1)\n",
    "outcome = df_reg.sumwages\n",
    "\n",
    "# Fit the model\n",
    "ols.fit(X = predictors, y = outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've fit our model, we can find the predicted values for earnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in the ssn_hash and predicted values\n",
    "missing_wages = pd.DataFrame({'ssn_hash':df_miss['ssn_hash'], 'sumwages':ols.predict(df_pred)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only looking at imputed values\n",
    "missing_wages.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after imputation\n",
    "wages_missing_regress = df_nona.loc[:,('ssn_hash', 'sumwages')].append(missing_wages)\n",
    "wages_missing_regress.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h4> Checkpoint 6: Include gender as a categorical variable and re-run the regression</h4></font> \n",
    "\n",
    "When you include gender as a categorical variable in the regression, how does the earnings distribution compare to the one using the previous linear regression to impute values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Add in Indiana, Missouri, and Illinois UI data\n",
    "\n",
    "Finally, let's see how the earnings distribution changes when we add in some bordering states' UI wage records. You will see how we joined Indiana, Missouri and Illinois' UI wage records to our `cc_grads_recent` table. Afterwards, we will combine these tables to analyze the overall earnings distribution.\n",
    "\n",
    "By adding in contiguous states' wage records, we should be able to capture most earnings of our cohort that were outside of Ohio.\n",
    "\n",
    "Recall that in the data exploration notebook, we created the permanent table `cohort_in_jobs` by joining `cc_grads_recent` to `small_indiana_ui`, which was a subset of the entire UI wage records for Indiana. The following SQL queries created `cohort_in_jobs`, `cohort_il_jobs`, and `cohort_mo_jobs`.\n",
    "\n",
    "> Note: `cohort_in_jobs`, `cohort_mo_jobs`, and `cohort_il_jobs` do not contain `degcert_subject` columns. Feel free to add them in yourself using `cc_grads_recent` and the corresponding UI wage record table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    create table ada_20_osu.cohort_in_jobs as\n",
    "    select a.ssn_hash, a.deg_date, b.job_date, b.sumwages, (b.job_date - a.deg_date) time_after_grad\n",
    "    from cc_grads_recent a\n",
    "    join ada_20_osu.small_indiana_ui b\n",
    "    on a.ssn_hash = b.ssn_hash\n",
    "    where b.job_date > a.deg_date AND (a.deg_date + '1 year'::interval) >= b.job_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    create table ada_20_osu.cohort_mo_jobs as\n",
    "    select a.ssn_hash, a.deg_date, b.job_date, sum(b.wage), (b.job_date - a.deg_date) time_after_grad\n",
    "    from cc_grads_recent a\n",
    "    join mo_small b\n",
    "    on a.ssn_hash = b.ssn\n",
    "    group by a.ssn_hash\n",
    "    where b.job_date > a.deg_date AND (a.deg_date + '1 year'::interval) >= b.job_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    create table ada_20_osu.cohort_il_jobs as\n",
    "    select a.ssn_hash, a.deg_date, b.job_date, sum(b.wage), (b.job_date - a.deg_date) time_after_grad\n",
    "    from cc_grads_recent a\n",
    "    join il_small b\n",
    "    on a.ssn_hash = b.ssn\n",
    "    group by a.ssn_hash\n",
    "    where b.job_date > a.deg_date AND (a.deg_date + '1 year'::interval) >= b.job_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly explore these tables to see how many `ssn_hash` values they captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in indiana table \n",
    "qry = '''\n",
    "select * \n",
    "from ada_20_osu.cohort_in_jobs\n",
    "'''\n",
    "in_df=pd.read_sql(qry, conn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see indiana table\n",
    "in_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of ssn_hash values found in Indiana\n",
    "in_df['ssn_hash'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missouri and Illinois only have by employer UI wage records, but this will not affect your analysis. You can still repeat the previous process, i.e., create a small UI table for each state, join them to `cc_grads_recent`, and then aggregate the data by `ssn_hash`, as shown in the table creation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you have the Missouri UI records for 2012-13 Ohio community college graduates.\n",
    "qry = '''\n",
    "select * \n",
    "from ada_20_osu.cohort_mo_jobs\n",
    "'''\n",
    "mo_df = pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see mo_df\n",
    "mo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of ssn_hash values\n",
    "mo_df['ssn_hash'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you have the Illinois UI records for 2012-13 Ohio community college graduates.\n",
    "qry = '''\n",
    "select * \n",
    "from ada_20_osu.cohort_il_jobs\n",
    "'''\n",
    "il_df = pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of ssn_hash values\n",
    "il_df['ssn_hash'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have access to four tables that have Ohio community college graduates' UI records from the four states. You can append the four tables by using `union` in SQL. You can also create a `state` column to track the state the individual found employment in during first quarter after graduation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join graduates employment outcomes in other states during first quarter after graduation\n",
    "#also included distinction by state\n",
    "qry = '''\n",
    "drop table if exists all_wages; \n",
    "create temp table all_wages as\n",
    "select distinct ssn_hash, deg_date, job_date, wage as sumwages, time_after_grad, 'il' as state\n",
    "from ada_20_osu.cohort_il_jobs\n",
    "where time_after_grad in (91, 92)\n",
    "union all\n",
    "select distinct ssn_hash, deg_date, job_date, sumwages, time_after_grad, 'oh' as state\n",
    "from ada_20_osu.cohort_oh_jobs\n",
    "where time_after_grad in (91, 92)\n",
    "union all\n",
    "select distinct ssn_hash, deg_date, job_date, wages as sumwages, time_after_grad, 'in' as state\n",
    "from ada_20_osu.cohort_in_jobs\n",
    "where time_after_grad in (91, 92)\n",
    "union all\n",
    "select distinct ssn_hash, deg_date, job_date, wage as sumwages, time_after_grad, 'mo' as state\n",
    "from ada_20_osu.cohort_mo_jobs\n",
    "where time_after_grad in (91, 92)\n",
    "'''\n",
    "conn.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table a look at the table.\n",
    "qry = '''\n",
    "select * from all_wages limit 5\n",
    "'''\n",
    "pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw when initially forming `df_q1`, `all_wages` does not include people who do not have incomes in the four states. You need to add these people back by joining `all_wages` with the `ssn_hash` values from `cc_grads_recent` that were not already captured in `all_wages`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''\n",
    "drop table if exists all_jobs_w_missing;\n",
    "create temp table all_jobs_w_missing as\n",
    "select ssn_hash, deg_date, NULL as job_date, NULL as sumwages, NULL as time_after_grad, 'No Record' as state\n",
    "from cc_grads_recent a\n",
    "where ssn_hash NOT IN (SELECT distinct ssn_hash from all_wages)\n",
    "UNION ALL\n",
    "select * from all_wages;\n",
    "'''\n",
    "conn.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the table\n",
    "qry = '''\n",
    "select * \n",
    "from all_jobs_w_missing\n",
    "'''\n",
    "cross_state_df = pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_state_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check how many people have earnings in each state.\n",
    "cross_state_df['state'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many people worked in one, two, three and four states!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First drop all duplicates 'ssn_hash' within same state column as `no_dup`\n",
    "no_dup = cross_state_df.loc[cross_state_df['state'].isin([\n",
    "    'il', 'mo', 'in', 'oh'])][['ssn_hash', 'state']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of jobs in different states by ssn_hash\n",
    "num_jobs = no_dup.groupby(['ssn_hash'])['state'].agg('count').reset_index(name='num_states')\n",
    "num_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get aggregate count by num_states\n",
    "num_jobs.groupby('num_states').agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how many missing values we have filled in by adding additional states' UI records. Note that some people have worked in more than one state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check how many more people have positive earnings now\n",
    "added_recs = cross_state_df[cross_state_df['sumwages']>0]['ssn_hash'].nunique() - df_q1['ssn_hash'].nunique()\n",
    "\n",
    "print('''\n",
    "By adding in UI wage records from a handful of bordering states, \n",
    "we have managed to find wage records for {} more people, \n",
    "as well as augmented earnings for some others. \n",
    "Let's see how this change affected the earnings distribution.\n",
    "'''.format(added_recs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the earnings distribution after we add UI records from other states\n",
    "cross_state_df.groupby(['ssn_hash'])['sumwages'].agg('sum').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Earnings Distributions\n",
    "\n",
    "We can quickly determine whether either or both imputation methods have significantly altered the pre-imputation wage distribution with visualization. Plotting side-by-side boxplots can be an effective choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with all the imputation methods outcomes\n",
    "df1 = cohort_oh_jobs_q1[['ssn_hash', 'sumwages']]\n",
    "df2 = wages_no_missing.rename(columns=({'sumwages':'earnings_no_imp'}))\n",
    "df3 = wages_zero.rename(columns=({'sumwages':'earnings_imp_zero'}))\n",
    "df4 = wages_missing_as_mean[['ssn_hash', 'imputed_wages']].rename(columns=({'imputed_wages':'earnings_imp_mean'}))\n",
    "df5 = wages_missing_regress.rename(columns=({'sumwages':'earnings_imp_regress'}))\n",
    "frames = [df1, df2, df3, df4, df5]\n",
    "result = pd.concat(frames, axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all distributions for one quarter after graduation side-by-side\n",
    "fig,ax = plt.subplots(figsize = (15, 8))\n",
    "result[['earnings_no_imp', 'earnings_imp_zero', \n",
    "    'earnings_imp_mean', 'earnings_imp_regress']].\\\n",
    "boxplot(grid = False, vert = False)\n",
    "ax.set(title = 'distribution of earnings one quarter after graduation',\n",
    "       yticklabels = ['no imputation', 'imputed zero', \n",
    "                      'imputed mean by gender and major', 'regression'],\n",
    "       xlim = (-500,11000),\n",
    "       xticks = (np.arange(0, 11000, 1000)))\n",
    "plt.annotate('Sources: OH HEI data and UI wage records', \n",
    "             xy=(0.75,-0.1), xycoords=\"axes fraction\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red\">Checkpoint 7: Visualizing cross state earnings</h3>\n",
    "Add the cross state earnings distribution to the above visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple histograms\n",
    "\n",
    "We can also look at the differences in the earnings distribution by looking at side-by-side histograms. To do so, we need to convert `result` from a wide (lots of columns, less entries) to a long format (less columns, lots of entries).\n",
    "\n",
    "Before we append each of the individual DataFrames, we need to make sure they all have the same columns. Let's also drop the `ssn_hash` columns from the DataFrames that have the column and instead rely on the `index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure columns are index, earnings, method\n",
    "df2['method'] = 'no imputation'\n",
    "df2.columns = ('index', 'earnings', 'method')\n",
    "df3['method'] = 'imputed zero'\n",
    "df3.columns = ('index', 'earnings', 'method')\n",
    "df4['method'] = 'imputed mean'\n",
    "df4.columns = ('index', 'earnings', 'method')\n",
    "df5['method'] = 'regression'\n",
    "df5.columns = ('index', 'earnings', 'method')\n",
    "df6 = cross_state_df.loc[:,('ssn_hash', 'sumwages')]\n",
    "df6['method'] = 'cross state'\n",
    "df6.columns = ('index', 'earnings', 'method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = df2.append(df3).append(df4).append(df5).append(df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare our grid, which will share axes across multiple plots (wrapping after 5 columns)\n",
    "g = sns.FacetGrid(result2, col='method',col_wrap=5)\n",
    "\n",
    "# Create a lineplot for each cell of the grid\n",
    "g = g.map(plt.hist, \"earnings\", color=\"lightcoral\")\n",
    "\n",
    "# Simplify the titles inside each cell\n",
    "g.set_titles(\"{col_name}\")\n",
    "\n",
    "plt.annotate(\n",
    "        'Sources: OH HEI data and UI wage records',\n",
    "        fontsize='x-small',\n",
    "        xycoords=\"figure fraction\", # specify x and y positions as % of the overall figure\n",
    "        xy=(1, 0.01), # 100% to the right (x) and 1% to the top (y) means bottom right\n",
    "        horizontalalignment='right')\n",
    "\n",
    "# Remove the spine (vertical line) along the y axis\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Advanced: Using machine learning to impute values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To impute values, we can also machine learning algorithms such as `K-nearest Neighbors` and `Decision Trees`. The principle behind `K-nearest Neighbors` is quite simple: the missing values can be imputed by values of \"closest neighbors\" - as approximated by other, known, features. \n",
    "\n",
    "For example, if we had cases where the data on earnings of some graduates was completely missing, we could approximate their earnings by referring to other characteristics which could be shared by major group (their 'closest neighbors' in terms of characteristics).\n",
    "\n",
    "The algorithm calculates the distance between the input values (the missing values) and helps to identify the nearest possible value based on other features (such as known characteristics of the closest major group). Imputing missing data using machine learning has become a research hotbed, and there are plenty of papers covering the various algorithms if you are curious."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-ada",
   "language": "python",
   "name": "py3-ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
