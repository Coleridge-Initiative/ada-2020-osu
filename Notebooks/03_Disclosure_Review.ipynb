{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"images/CI_horizontal.png\" width=\"600\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n",
    "\n",
    "Ghani, Rayid, Frauke Kreuter, Julia Lane, Brian Kim, Adrianne Bradford, Alex Engler, Nicolas Guetta Jeanrenaud, Graham Henke, Daniela Hochfellner, Clayton Hunter, Avishek Kumar, Jonathan Morgan, Benjamin Feder, Ekaterina Levitskaya, Nathan Caplan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Disclosure Review Examples & Exercises_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides you with information on how to prepare research output for disclosure control. It outlines how to prepare different kind of outputs before submitting an export request and gives you an overview of the information needed for disclosure review. _Please read through the entire notebook because it will separately discuss different types of outputs that will be flagged in the disclosure review process._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# database connection\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Connection\n",
    "host = 'stuffed.adrf.info'\n",
    "DB = 'appliedda'\n",
    "\n",
    "connection_string = \"postgresql://{}/{}\".format(host, DB)\n",
    "conn = create_engine(connection_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Remarks on Disclosure Review\n",
    "\n",
    "## Files you can export\n",
    "In general, you can export any kind of file format. However, most researchers typically export tables, graphs, regression outputs and aggregated data. Thus, we ask you to export one of these types, which implies that every result you would like to export needs to be saved in either .csv, .txt or graph format.\n",
    "\n",
    "## Jupyter notebooks are only exported to retrieve code\n",
    "Unfortunately, you can't export results in a Jupyter notebook. Doing disclosure reviews on output in Jupyter notebooks is too burdensome for us. Jupyter notebooks will only be exported when the output is deleted for the purpose of exporting code. **This does not mean that you won't need your Jupyter notebooks during the export process.** \n",
    "\n",
    "## Documentation of code is important\n",
    "During the export process, we ask you to provide the code for every output you would like to export. It is important for the ADRF staff to have the code to better understand what you exactly did. Understanding how research results are created is important in understanding your research output. Thus, it is important to document every step of your analysis in your Jupyter notebook. \n",
    "\n",
    "## General rules to keep in mind\n",
    "A more detailed description of the rules for exporting results can be found on the class website. This is just a quick overview. You should go to the class website and read the entire guidelines (link below) before preparing your files for export. \n",
    "- The disclosure review is based on the underlying observations of your study. **Every statistic you want to export must be based on at least 10 data points at an individual level. When reporting firm-based statistics, on top of the 10 individual data points, you must show that there are at least 3 firms and 2) there are 3 or more firms, and employment in no one firm comprises more than 80% of the industry to receive your export**. You must show the disclosure review team that every statistic you wish to export is based on those numbers by providing the associated counts/percentages in your input file. \n",
    "- Document your code so the reviewer can follow your data work. Assessing re-identification risks highly depends on the context. Therefore, it is important that you provide context info with your analysis for the reviewer. When making a comments in the code, make sure not to use any individual statistic (e.g. the median is ...).\n",
    "- Save the requested output with the corresponding code in your input and output folder. Make sure the code is executable. The code should exactly produce the output you requested.\n",
    "- If you are exporting powerpoint slides that show project results, you have to provide the code which produces the output in the slide.\n",
    "- Please export results only when they are final and you need them for your presentation or final project report.\n",
    "\n",
    "## To-Do:\n",
    "Read through the **documentation** link: adrf.readthedocs.io/en/latest/export_of_results/guidelines.html#documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclosure Review Walkthrough\n",
    "\n",
    "You will reconstruct the statistics and visualizations you created in the [Data Exploration](01_2_Data_Exploration.ipynb) and [Data Visualization](02_1_Data_Visualization.ipynb) notebooks and prepare them in a way so your output will pass the disclosure review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall Motivating Question #1 from the Data Exploration notebook.\n",
    "\n",
    "**How many students got their degrees from Ohio community colleges during the 2012-13 academic year? How does the number vary by the regional location of the college and by degree field?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find these answers, you first found your desired cohort using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store query to find 2012-13 academic year graduates in a temporary table\n",
    "qry = '''\n",
    "create temp table all_grads as\n",
    "select *\n",
    "from data_ohio_olda_2018.oh_hei_long\n",
    "where (degcert_yr_earned = '2012' and (degcert_term_earned = '4' or degcert_term_earned = '1')) or \n",
    "    (degcert_yr_earned = '2013' and (degcert_term_earned = '2' or degcert_term_earned = '3'))\n",
    "'''\n",
    "conn.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create temp table because this is our cohort of 2012-13 community college graduates\n",
    "# take most recent graduation\n",
    "qry = '''\n",
    "create temp table cc_grads as\n",
    "select a.*, lkp.*\n",
    "from all_grads a\n",
    "left join data_ohio_olda_2018.oh_hei_campus_county_lkp lkp\n",
    "on a.degcert_campus = lkp.campus_num\n",
    "where lkp.campus_type_code in ('TC', 'SC', 'CC')\n",
    "'''\n",
    "conn.execute(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, finding the number of students who received community college degrees was done using `unique()`. Because the desired statistic for export is a count not related to firm statistics, you just need to show that this count is greater than 10.\n",
    "> If you were exporting a non-count, yet general statistic (i.e. total number of dollars made by this cohort in a year's time), you would need to provide the underlying individual counts per group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''\n",
    "select count(distinct(ssn_hash)) from cc_grads\n",
    "'''\n",
    "grad_count = pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grad_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export this statistic as a csv, you can use the `to_csv` function and designate the file path and the name. Here, we will call the file `graduate_counts.csv` (the more descriptive the name of the file, the easier it is to review).\n",
    "\n",
    "> In the file path, include `YOUR_USERNAME` to save the CSV in your home folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_count.to_csv('/nfshome/YOUR_USERNAME/graduate_counts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the number of graduates by region, recall that you used Python's `.groupby()` combined with `nunique()`. Again, because this count does not concern firm statistics, it can serve as its own proof that the counts are all at least 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry =  '''\n",
    "select *\n",
    "from cc_grads\n",
    "'''\n",
    "df = pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of graduates by region\n",
    "df.groupby(['jobsohioregion'])['ssn_hash'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, because these counts are all at least 10, they are safe for export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_by_region = df.groupby(['jobsohioregion'])['ssn_hash'].nunique()\n",
    "grad_by_region.to_csv('nfshome/YOUR_USERNAME/graduates_by_region_counts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish up the first motivating question, you found the number of graduates by two-digit subject codes. Let's see what the output looked like again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with creates a mini table\n",
    "# In the last line of the query, we use ::varchar to convert 'subject_code' in the lkp table from integer\n",
    "# to text. This is because when we join tables, the variable types should be the same. \n",
    "qry= '''\n",
    "with subject as (select ssn_hash, left(degcert_subject,2) as code from cc_grads)\n",
    "select subject.ssn_hash, lkp.subject_code_2010, lkp.subject_desc \n",
    "from subject\n",
    "join data_ohio_olda_2018.oh_subject_codes_lkp lkp\n",
    "on subject.code=lkp.subject_code_2010::varchar; \n",
    "'''\n",
    "\n",
    "subject_df=pd.read_sql(qry,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df.groupby(['subject_code_2010', 'subject_desc'])['ssn_hash'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have two options:\n",
    "1. Only export those with counts of at least 10.\n",
    "2. Aggregate the subjects with counts less than 10 so that all counts are at least 10.\n",
    "\n",
    "Here, you will see how to only export those with counts of at least 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as dataframe to easily manipulate\n",
    "df_counts_by_subject = pd.DataFrame(subject_df.groupby(['subject_code_2010', 'subject_desc'])['ssn_hash'].count().\n",
    "                                    sort_values(ascending=False)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit to subjects with at least 10 graduates\n",
    "df_counts_by_subject[df_counts_by_subject['ssn_hash'] >= 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table is now good to export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit to subjects with at least 10 graduates\n",
    "counts_by_subject = df_counts_by_subject[df_counts_by_subject['ssn_hash'] >= 10]\n",
    "counts_by_subject.to_csv('nfshome/YOUR_USERNAME/counts_by_subject.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move onto Motivating Question #2:\n",
    "\n",
    "**How many 2012-13 Ohio community college graduates are employed in Ohio one year after graduation? How many of them have stable employment? How does the number vary by industry?**\n",
    "\n",
    "And more specifically:\n",
    "\n",
    "- How many people have positive earnings each quarter during their first year after graduation?\n",
    "- What are the earning distributions within a year's time of graduates who have positive earnings during the first year after graduation?\n",
    "- How many people achieved stable employment within the first year after graduation? \n",
    "    - **Stable employment metric 1**: have positive earnings during ALL four quarters after graduation\n",
    "    - **Stable employment metric 2**: work for the same employer during the second quarter and the fourth quarter after graduation\n",
    "- How does the number of people who have stable employment vary by industry?\n",
    "\n",
    "To answer these questions, you utilized the `cohort_oh_jobs` table available in the `ada_20_osu` schema. So first, let's load the `cohort_oh_jobs` table into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to jobs within 1 year *after* graduation\n",
    "\n",
    "qry = '''\n",
    "select * from ada_20_osu.cohort_oh_jobs\n",
    "'''\n",
    "\n",
    "df_jobs = pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the number of graduates who had positive earnings in any quarter one year after graduation, you used the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many people had wages for at least one quarter\n",
    "df_jobs['ssn_hash'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this statistic's count is greater than 10 and does not concern specific employers, you do not need to show firm counts or firm employment percentages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the second subquestion, which concerns an earning distribution, for the last part of exporting this motivating question since it covers an important topic: fuzzy percentiles. In the meantime, let's move on to exporting stable employment metrics. Here, you will work with count statistics that do not include industry or regional breakdowns, so you don't need to worry about showing firm-specific information.\n",
    "\n",
    "As a reminder, the first stable employment metric you used was finding those who had positive earnings all four quarters their first year after graduation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stable employment metric 1:\n",
    "# positive earnings during all four quarters\n",
    "stable_1 = sum(df_jobs.groupby(['ssn_hash']).count()['sumwages'] == 4)\n",
    "stable_1.to_csv('/nfshome/YOUR_USERNAME/stable_emp_count_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second stable employment metric you used another table we created in the `ada_20_osu` schema, `cohort_oh_jobs_emp`. This table allows you to find the count for the second stable employment metric, which measured the amount of individuals who had the same primary employer in their second and fourth quarters after graduation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all jobs 2 and 4 quarters after graduation\n",
    "qry = '''\n",
    "select ssn_hash, deg_date, job_date, employer, naics_3_digit\n",
    "from ada_20_osu.cohort_oh_jobs_emp\n",
    "where time_after_grad between 180 and 185 or time_after_grad = 365\n",
    "'''\n",
    "stable_emp = pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the amount that had only one employer and showed up in stable_emp twice\n",
    "stable_2 = sum((stable_emp.groupby(['ssn_hash'])['employer'].nunique() == 1) & \n",
    "    (stable_emp.groupby(['ssn_hash']).count()['employer'] == 2))\n",
    "stable_2.to_csv('nfshome/YOUR_USERNAME/stable_emp_count_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've found and exported these counts, let's compare the different stable employment metrics by their top 10 industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''\n",
    "select *\n",
    "from ada_20_osu.cohort_oh_jobs_emp\n",
    "'''\n",
    "emp_df = pd.read_sql(qry, conn)\n",
    "\n",
    "emp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the ssn_hash of people who have four quarters of records\n",
    "ssn_4q_df=emp_df.groupby(['ssn_hash'])['wages'].agg(['count']).reset_index()\n",
    "ssn_4q_df=ssn_4q_df[ssn_4q_df['count']==4]\n",
    "\n",
    "#Merge this with emp_df to get industry code\n",
    "emp_4q_df=ssn_4q_df.merge(emp_df,left_on='ssn_hash',right_on='ssn_hash')\n",
    "\n",
    "#Keep the first quarter records only\n",
    "emp_4q_df=emp_4q_df[emp_4q_df['time_after_grad']<=92]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find top 10 industries\n",
    "sort_ind = emp_4q_df.groupby(['naics_3_digit'])['ssn_hash'].count().sort_values(ascending=False)\n",
    "sort_ind.iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all jobs 2 and 4 quarters after graduation\n",
    "qry = '''\n",
    "select ssn_hash, deg_date, job_date, employer, naics_3_digit\n",
    "from ada_20_osu.cohort_oh_jobs_emp\n",
    "where time_after_grad between 180 and 185 or time_after_grad = 365\n",
    "'''\n",
    "stable_emp = pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the dataframe of people who worked for the same employer during the 2nd and the 4th quarter after graduation\n",
    "stable_df=stable_emp.groupby(['ssn_hash','employer','naics_3_digit']).count().reset_index()\n",
    "stable_df = stable_df[stable_df['job_date']==2]\n",
    "\n",
    "#breakdown the number by industry\n",
    "sort_ind2=stable_df.groupby(['naics_3_digit'])['ssn_hash'].count().sort_values(ascending=False)\n",
    "sort_ind2.iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare the number of stable employment defined by the two metrics\n",
    "# can use same df because same top 10\n",
    "compare_df=pd.concat([sort_ind.iloc[0:10],sort_ind2.iloc[0:10]],axis=1).reset_index()\n",
    "compare_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, even though this statistic is broken down by industry breakdowns, because it does not concern specific employers, it is ready for export. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df.to_csv('nfshome/YOUR_USERNAME/stable_emp_by_industry.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to answer the second subquestion on the earning distribution, you used outputs from the `.describe()` function. However, you cannot use these outputs because some of those statistics are represented by individual points (such as minimum, maximum, any percentiles, and median). Instead, you need to create _fuzzy percentiles_. For example, in order to find a fuzzy 25th percentile, you can take the average of the 20th and 30th percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of wages per person one year out\n",
    "df_jobs.groupby(['ssn_hash'])['sumwages'].agg('sum').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy percentiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through the code to create the fuzzy percentiles. You can use the `.quantile()` function to find the true values for some percentiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that you want to export the 25th, 50th, and 75th percentiles. You can start by finding the following true percentiles on our weighted data:\n",
    "- 20th and 30th (to create a fuzzy 25th percentile),\n",
    "- 45th and 55th (to create a fuzzy 50th percentile),\n",
    "- 70th and 80th percentile (to create a fuzzy 75th percentile). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save distribution of annual wages per graduate\n",
    "wages = df_jobs.groupby(['ssn_hash'])['sumwages'].agg('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find 20, 30, 45, 55, 70, 80 percentiles\n",
    "wage_qntl = wages.quantile([.20, .30, .45, .55, .70, .80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_qntl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's average the percentiles to create fuzzy 25th, 50th, and 75th percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find values for the fuzzy quantiles by averaging the percentiles \n",
    "# (e.g. to find 25th, average 20th and 30th, etc.)\n",
    "\n",
    "fp_25 = str((wage_qntl[.20] + wage_qntl[.30])/2)\n",
    "fp_50 = str((wage_qntl[.45] + wage_qntl[.55])/2)\n",
    "fp_75 = str((wage_qntl[.70] + wage_qntl[.80])/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save these fuzzy percentiles to a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in pandas dataframe\n",
    "\n",
    "fuzzy = pd.DataFrame()\n",
    "fuzzy['percentile'] = ['fuzzy_25', 'fuzzy_50', 'fuzzy_75']\n",
    "fuzzy['wages'] = [fp_25, fp_50, fp_75]\n",
    "\n",
    "fuzzy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, these percentiles describing the wage distribution of your cohort are safe for export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy.to_csv('/nfshome/YOUR_USERNAME/fuzzy_female_earnings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Data Visualization notebook, Motivating Question #1 is as follows:\n",
    "\n",
    "**What is the distribution of earnings during the first year after graduation for 2012-13 community college graduates? How does this differ by degree fields?**\n",
    "\n",
    "To answer these questions, you created variations of histograms, first starting with a simple visualizaiton and then getting more advanced. How would you submit these visualizations for export? Is there anything else you would need to provide? Let's start with a histogram of earnings during the first year after graduation for 2012-13 community college students in Ohio. `df_jobs` already contains everything you need for the first histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bare histogram of earnings distribution\n",
    "plt.hist(df_jobs.groupby(['ssn_hash'])['sumwages'].agg('sum'))\n",
    "\n",
    "# The show() function outputs the current state of `pyplot`: our current fig.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual `plt.hist()` call has three outputs, which pertain to the counts of the bin sizes, the edges of the bins, and the actual graphical image. You need to show that each bin contains at least 10 individual data points before you can export this histogram. The code cell below shows you how you can find the counts per bin. Here, we are also assuming you have chosen the default number of bins, but the code can be extended to when you change the number of bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, edges, graph = plt.hist(df_jobs.groupby(['ssn_hash'])['sumwages'].agg('sum'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if all bin counts are greater than 10\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can adjust the bin size by either aggregating the smaller bins into one larger bin that satisfies the disclosure review process, or cutting off outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see bin edges\n",
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change edges so that the counts are okay\n",
    "counts, edges, graph = plt.hist(df_jobs.groupby(['ssn_hash'])['sumwages'].agg('sum'), bins = [REDACTED])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check counts\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all the bin counts are at least 10, so this histogram is safe to export. First, let's export the histogram by using the `.savefig()` function, which works similarly to `to_csv()`.\n",
    "> You cannot save a figure directly after running `plt.show()`. To save the figure, you need to run the plot and then `.savefig()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('/nfshome/YOUR_USERNAME/earnings_hist.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just need to also report the accompanying bin counts stored in `counts`. Please make sure the counts exports for visualizations are easy to link with your visualizations by naming them `counts_for_...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.to_csv('/nfshome/YOUR_USERNAME/counts_for_earnings_hist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to export visualizations containing multiple groups, such as the subquestion regarding earnings differences by degree field, you need to show the counts within each group. Let's recall the code used to generate that visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most recent graduation within the span of 2012-13 academic year\n",
    "# also get two-digit subject code\n",
    "qry = '''\n",
    "create temp table cc_grads_recent as\n",
    "select distinct on (ssn_hash) *, left(degcert_subject, 2) as subject\n",
    "from (\n",
    "SELECT *, \n",
    "    CASE WHEN degcert_term_earned = 4 THEN\n",
    "        format('%%s-%%s-01', degcert_yr_earned, 7)::date \n",
    "    WHEN degcert_term_earned = 1 THEN\n",
    "        format('%%s-%%s-01', degcert_yr_earned, 10)::date \n",
    "    WHEN degcert_term_earned = 2 THEN\n",
    "        format('%%s-%%s-01', degcert_yr_earned, 1)::date \n",
    "    WHEN degcert_term_earned = 3 THEN\n",
    "        format('%%s-%%s-01', degcert_yr_earned, 4)::date \n",
    "    END AS deg_date\n",
    "    from cc_grads\n",
    ") q\n",
    "order by ssn_hash, deg_date DESC\n",
    "'''\n",
    "conn.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subject'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select these subjects so we can subset most_recent and add the corresponding subject description\n",
    "# need to set as tuple so we can use .format() properly\n",
    "pop_subs = tuple(subject_df.groupby(['subject_code_2010'])['ssn_hash'].count().sort_values(\n",
    "    ascending=False)[0:10].reset_index()['subject_code_2010'])\n",
    "\n",
    "pop_subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as temp table ten_subs\n",
    "qry= '''\n",
    "create temp table ten_subs as\n",
    "select cc.ssn_hash, cc.deg_date, cc.subject, lkp.subject_desc \n",
    "from cc_grads_recent cc\n",
    "join data_ohio_olda_2018.oh_subject_codes_lkp lkp\n",
    "on cc.subject=lkp.subject_code_2010::varchar\n",
    "where cc.subject != 'TR' and cc.subject::int in {}\n",
    "'''.format(pop_subs)\n",
    "conn.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have this, we can match it to the cohort_oh_jobs table because it already contains the earnings\n",
    "# for most recent graduation within this time\n",
    "qry = '''\n",
    "select distinct t.*, j.deg_date, j.sumwages\n",
    "from ten_subs t\n",
    "join ada_20_osu.cohort_oh_jobs j\n",
    "on j.ssn_hash = t.ssn_hash\n",
    "'''\n",
    "top_subs_wage = pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_subs_wage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate each person's earnings during the first year after graduation\n",
    "df_by_ssn = top_subs_wage.groupby(['ssn_hash', 'subject_desc'])['sumwages'].agg('sum').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_ssn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('figure', figsize=(15, 10))\n",
    "\n",
    "# By convention, a returned Axes object is often called `ax`\n",
    "ax = sns.barplot(\n",
    "    y=\"subject_desc\", # seaborn is clever enough to create a horizontal chart\n",
    "    x=\"sumwages\", \n",
    "    data=df_by_ssn, # order in data to order in figure\n",
    "    palette='vlag',\n",
    "    ci=None\n",
    ")\n",
    "\n",
    "ax.set_title('First Year Earnings Varies Considerably Across Degree Fields');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can safely export this visualization, though, we need to show individual counts for each subject field.\n",
    "> This policy would also follow for a line graph. Say you wanted to export earnings over time for your cohort, you would need to show counts for each division of time in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get individual counts by subject field, we need to join two tables we've already made: \n",
    "- `ada_20_osu.cohort_oh_jobs`, which has UI wage records by quarter information\n",
    "- `ten_subs`, which has subject information and limits cohort to those with degrees in our subjects of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''\n",
    "select j.*, t.subject, t.subject_desc\n",
    "from ada_20_osu.cohort_oh_jobs j\n",
    "join ten_subs t\n",
    "on t.ssn_hash = j.ssn_hash and t.deg_date = j.deg_date\n",
    "'''\n",
    "df_sub = pd.read_sql(qry, conn)\n",
    "\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one example that will motivate a loop\n",
    "df_sub[df_sub['subject'] == '11']['ssn_hash'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a `for()` loop to find counts and percentages by each subject field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's get counts for each subject\n",
    "count_stat1 = list()\n",
    "for code in pop_subs:\n",
    "    code = str(code)\n",
    "    count_stat1.append(df_sub[df_sub['subject'] == code]['ssn_hash'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discl_proof = pd.DataFrame({'subject_code':pop_subs, 'individual_counts':count_stat1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discl_proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have provided the necessary statistics for export, we can export the visualization and the corresponding counts per subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discl_proof.to_csv('nfshome/YOUR_USERNAME/earnings_hist_spring_counts.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('/nfshome/YOUR_USERNAME/earnings_hist_spring.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you are creating your training and test datasets, after creating them, please include the counts of each variable, and please do not alter the datasets afterwards. If you use any dummy variables, you need to provide the countof 0s and 1s for each dummy variable. This will be covered more extensively in the unsupervised machine learning notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that if you are plotting y-scores, it is still a histogram, and each estimate represents an individual data point, therefore, it needs to comply with the disclosure threshold described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder\n",
    "Every single item you wish to export, regardless of whether it is a .csv, .pdf, .png, or something else, must have corresponding proof in your input file to show that every group used to create this statistic followed our disclosure review rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: After the end of the course, you can export the code that you have been using. In order to do that, you will need to clear the outputs of the notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-ada",
   "language": "python",
   "name": "py3-ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
