{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"images/CI_horizontal.png\" width=\"1000\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Julia Lane, Benjamin Feder, Tian Lou, Lina Osorio-Copete </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are problems where there does not exist a target variable that we want to predict but we want to understand \"natural\" grouping or patterns in the data. Unsupervised machine learning methods are used to tackle these problems. Clustering is the most common unsupervised machine learning technique, but you might also be aware of neural networks or principal components analysis (PCA). This notebook will provide an introduction to unsupervised machine learning through a clustering example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is used to group data points together that are similar to each other. Optimally, a given clustering method will produce groupings with high intra-cluster (within) similarity and low inter-cluster (between) similarity. Clustering algorithms typically require a distance or similarity metric to generate clusters. They take a dataset and a distance metric (and sometimes additional parameters), and they generate clusters based on that distance metric. The most common distance metric used is Euclidean distance, but other commonly used metrics are Manhattan, Minkowski, Chebyshev, cosine, Hamming, Pearson, and Mahalanobis.\n",
    "\n",
    "Most clustering algorithms also require the user to specify the number of clusters (or some other parameter that indirectly determines the number of clusters) in advance as a parameter. This is often difficult to do a priori and typically makes clustering an iterative and interactive task. Another aspect of clustering that makes it interactive is often the difficulty in automatically evaluating the quality of the clusters. While varioius analytical clustering metrics have been developed, the best clustering is task-dependent and thus must be evaluated by the user. There may be different clusterings that can be generated with the same data. You can imagine clustering similar news stories based on the topic content, based on the writing style or based on sentiment. The right set of clusters depends on the user and the task they have. Clustering is therefore typically used for exploring the data, generating clusters, exploring the clusters, and then rerunning the clustering method with different parameters or modifying the clusters (by splitting or merging the previous set of clusters). Interpreting a cluster can be nontrivial: you can look at the centroid of a cluster, look at frequency distributions of different features (and compare them to the prior distribution of each feature).\n",
    "\n",
    "Here, we will focus on **K-Means clustering** (*k* defines the number of clusters), which is considered to be the most commonly used clustering method. The algorithm works as follows:\n",
    "1. Select *k* (the number of clusters you want to generate).\n",
    "2. Initialize by selecting k points as centroids of the *k* clusters. This is typically done by selecting k points uniformly at random.\n",
    "3. Assign each point a cluster according to the nearest centroid.\n",
    "4. Recalculate cluster centroids based on the assignment in **(3)** as the mean of all data points belonging to that cluster.\n",
    "5. Repeat **(3)** and **(4)** until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm stops when the assignments do not change from one iteration to the next. The final set of clusters, however, depends on the starting points. If initialized differently, it is possible that different clusters are obtained. One common practical trick is to run *k*-means several times, each with different (random) starting points. The *k*-means algorithm is fast, simple, and easy to use, and is often a good first clustering algorithm to try and see if it fits your needs. When the data are of the form where the mean of the data points cannot be computed, a related method called *K-medoids* can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will use *k*-means clustering to better understand Ohio's labor market in the third quarter of 2013. We've already developed a handful of employer-level measures in the [Data Preparation](ML_Data_Prep.ipynb) notebook. We will try a few different values of *k* to see how we can best understand the labor market by looking for differentiation between each of the clusters.\n",
    "\n",
    "We are hoping to identify clusters with \"optimal employers,\" and then use our cohort of 2012-13 Ohio community college graduates from previous notebooks to see who is finding employment in these better clusters. What industries do these \"better\" employers tend to come from? Are community college graduates finding jobs with these employers? Are there earnings outcomes what we would expect? Essentially, we will be using *k*-means clustering to help bolster our sense of the various associate's degrees to employment pathways, and which ones happen to be more successful than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages and Set Up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas-related imports\n",
    "import pandas as pd\n",
    "\n",
    "# Numpy\n",
    "import numpy as np\n",
    "\n",
    "# database interaction imports\n",
    "import sqlalchemy\n",
    "\n",
    "# import viz \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to create a connection to the database, \n",
    "# we need to pass the name of the database and host of the database\n",
    "\n",
    "host = 'stuffed.adrf.info'\n",
    "DB = 'appliedda'\n",
    "\n",
    "connection_string = \"postgresql://{}/{}\".format(host, DB)\n",
    "conn = sqlalchemy.create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, we have created a DataFrame that contains employer IDs, industry codes, and 11 employer characteristics and saved it in a .csv file. Let's use `read_csv` to import the data to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "emp = pd.read_csv('employer_all_new.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the DataFrame\n",
    "emp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you do not use `index_col = 0` when you read in the .csv file, you should see an extra column `Unnamed:0`, which contains the row numbers as an additional column. The extra column is generated when running `to_csv`, which will morph a given DataFrame into a .csv file. We need to remove it when we read in the data. Alternatively, you can set the `index` argument to `False` to avoid creating this additional column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Statistics\n",
    "emp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to remove the `employer` variable from our DataFrame since these features do not provide any explanatory power for our k-means algorithm. Additionally, k-means algorithms only work properly with continuous features. This is because k-means calculates its distance measure using euclidean distance, which is the distance between each data point and the centroid of a cluster. It is hard to assign positions for categorical variables in the euclidean space. Thus, we also need to remove `naics_3_digit` from `emp`.\n",
    "\n",
    "> There are more complicated clustering algorithms that do not use Euclidean distances and thus allow categorical variables in the model. If you are interested in them, you can take a look at: <a href='https://pypi.org/project/kmodes/'>k-modes</a> and <a href='https://thinkdatascience.com/post/2019-12-16-introduction-python-package-gower/'>gower</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of employer and naics for clustering\n",
    "emp_ml = emp.drop(['employer', 'naics_3_digit'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a look at the DataFrame now. It only has the 11 employer characteristics. \n",
    "#We will use them as the features in the k-means clustering.\n",
    "emp_ml.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure every column is numeric\n",
    "emp_ml.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choose the Number of Clusters, *K*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a *k*-means model is simple: we just need to use `KMeans()` and choose the number of clusters `n_clusters`. What number should we choose? Here, we have 11 features, so it is hard to visualize the data and decide the proper number by using our eyes. Let's start with a small number, such as 3, and see how the results look like.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because *k*-means clustering will generate different results (due to different starting points), we will set a seed so that the work in this notebook can be reproducible using the `random_state` argument in `KMeans()`. To get the same results, you must use the same seed before running the clustering algorithm every time. Luckily, if you set the same seed as your collaborators and are running the same *k*-means algorithm, you will see the same results, even if you are working in different environments, i.e. Jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are additional parameters in the `KMeans()` function that you can adjust and tune them based on your assumptions and knowledge about the sample and the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "kmeans = KMeans(n_clusters = 3, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the model on `emp_ml`. **It is important that we scale the features** before we compute *k*-means clustering, especially because the metrics are on a variety of numerical scales. Here, we can just use `scale(emp_ml)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model \n",
    "kmeans.fit(scale(emp_ml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict which cluster each employer belongs to\n",
    "y_means = kmeans.predict(scale(emp_ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have run your first *k*-means clustering model! Let's put the results and features into a DataFrame, `frame_3`, so that we can analyze the attributes of employers in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put employer IDs, industry codes, and the eleven features into frame_3.\n",
    "frame_3 = pd.DataFrame(emp)\n",
    "\n",
    "# Add the predicted clusters into frame_3 as well.\n",
    "frame_3['cluster'] = y_means\n",
    "\n",
    "# Let's see how many employers are in each cluster.\n",
    "frame_3['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of the employers are concentrated in cluster 1. In the perfect world, we would want them to be distributed more evenly across clusters, but in some cases, it may make sense that they wouldn't. Most importantly, we are looking for high intra-cluster similarity and low inter-cluster similarity.\n",
    "\n",
    "Are there major differences in the characteristics of employers in each cluster? We have created a function `characteristics_table` for you. It tells you whether the cluster average of a variable is higher **(+)** or lower **(-)** than the full sample average of that variable. To use the function, you need two arguments: 1) `cols`, a list of employer characteristics you want to look at; 2) `cframe`, the DataFrame you have created in the previous step, `frame_3`, which contains employer characteristics and the cluster each employer belongs to. The function returns a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we want to include the number of employees within each cluster in our tables so that we do not \n",
    "# need to go back and check the numbers every time when we look at the table.\n",
    "\n",
    "# Here, the mini function get_counts returns a DataFrame that has the cluster indicators and number of \n",
    "# employers within each cluster.\n",
    "\n",
    "def get_counts(frame):\n",
    "    c_frame = frame.groupby(['cluster'])['employer'].agg('count').reset_index().rename(columns={\"employer\":\"n\"})\n",
    "    return c_frame\n",
    "\n",
    "def characteristics_table(cols,cframe):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates an employer characteristics comparison table.\n",
    "    If the mean of a cluster is greater than the mean of all employers, labelled by '+'.\n",
    "    If the mean of a cluster is less than the mean of all employers, labelled by '-'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cols: a list of employer characteristics\n",
    "    cframe: a DataFrame contains the employer characteristics listed in cols \n",
    "            and the predicted cluster of each employer\n",
    "    \"\"\"\n",
    "    \n",
    "    cframe_no_cols = cframe[cols] \n",
    "    \n",
    "    all_df = get_counts(cframe)\n",
    "    \n",
    "    for var in cols:\n",
    "        all_df = all_df.merge((cframe.groupby(['cluster'])[var].agg('mean') > \n",
    "                              cframe[var].mean()).reset_index(), on = 'cluster')\n",
    "        \n",
    "    all_df[cols] = all_df[cols].replace({False:'-',True:'+'})\n",
    "    \n",
    "        \n",
    "    return all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of employer characteristics\n",
    "# Here, we just include all the features we used for the clustering model\n",
    "cols = emp_ml.columns.tolist()\n",
    "\n",
    "# Run the function and save the results of the function to a DataFrame\n",
    "c3_emp_charcs = characteristics_table(cols,frame_3)\n",
    "\n",
    "# Let's see how the table looks like!\n",
    "c3_emp_charcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our biggest cluster, *cluster 1*, contains relatively small employers that pay their employees low wages and have high job turnover rates. *cluster 0* also has relatively small employers, but on average, they pay their employees more and have a lower job turnover rate. *cluster 2* are high-quality, larger employers.\n",
    "\n",
    "However, in Ohio, many big companies have a large number of low-wage workers and high job turnover rates, such as big grocery stores and supermarket franchises. Clearly, our three-cluster model did not properly capture these companies. Let's increase k to see if it improves our results.\n",
    "\n",
    "We will set k=7 and see how the employer distributions across clusters change.\n",
    "\n",
    "> The Quarterly Census of Employment and Wages (QCEW) data has even more detailed information about employers, such as whether an employer is single unit or multi-unit, the location of the employer, and the monthly employment, etc. Some of these variables are crucial for the clustering model. For example, we might want to treat each unit of a multi-unit employer as a separate employer or treat single- and multi-unit employers differently, because different sites of a multi-site employer may have different industries sometimes. Amazon Inc. headquarters may be listed in Information sector (NAICS code: 51), while one of its warehouses in Ohio may be in the Retail Trade sector (NAICS code: 45). As another example, Davis and Haltiwanger (1992) pointed out that job desctruction and creation rates are different for single-site and multi-site employers. Because of these potential differences between single-site and multi-site employers, when we are running the clustering model, they may end up in different clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "kmeans = KMeans(n_clusters = 7, random_state = 42)\n",
    "\n",
    "# Fit the model\n",
    "kmeans.fit(scale(emp_ml))\n",
    "\n",
    "# Predict which cluster each employer belongs to\n",
    "y_means = kmeans.predict(scale(emp_ml))\n",
    "\n",
    "# Put employer IDs, industry codes, and the eleven features into frame_7\n",
    "frame_7 = pd.DataFrame(emp)\n",
    "\n",
    "# Add the predicted clusters into frame_7 as well\n",
    "frame_7['cluster'] = y_means\n",
    "\n",
    "# Let's see how many employers are in each cluster\n",
    "frame_7['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, let's see if we get more diversed clusters with the employer\n",
    "cols = emp_ml.columns.tolist()\n",
    "\n",
    "# Run the function and save the results of the function to a DataFrame\n",
    "c7_emp_charcs = characteristics_table(cols,frame_7)\n",
    "\n",
    "# Let's see how the table looks like!\n",
    "c7_emp_charcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the seven-cluster model, the employers are distributed more evenly across clusters. We see three clusters of high-quality big employers (high earnings, many full-quarter employees, low turnover rates), one cluster of high-quality small employers, one cluster of average-quality small employers, and two clusters of low-quality small employers. Although we find more diversified clusters, the information conveyed in our +/- table is not enough for us to tell more differences across clusters. In a later section, we will create more robust employer tables and use boxplots to get better inter-cluster details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of this problem, do you think seven is the optimal amount of clusters? If not, how many clusters should we choose? We can use the *Elbow method* to select the optimal cluster number. Recall that *k*-means starts with k random cluster centers (centroids), assigns each data point to the closest centroid, and calculates the distances between each point and the centroid. Then it moves the positions of the centroids and repeats the previous steps until there is convergence. In the *Elbow method*, we try different k values and calculate the sum of squared errors (`SSE`) after the model converges. Then we plot all the `SSE` by K in a line-chart. The line-chart should resemble an arm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the Elbow Curve!\n",
    "\n",
    "sse = [1] # sum of squared error\n",
    "delta = [0] # change in SSE\n",
    "percent = [0] # percentage change in SSE \n",
    "\n",
    "# Run the K-Means Clustering model for K= 1 to 15\n",
    "for cluster in range(1, 15):\n",
    "    kmeans = KMeans(n_clusters = cluster, random_state=42)\n",
    "    kmeans.fit(scale(emp_ml))\n",
    "    sse.append(kmeans.inertia_)\n",
    "    delta.append(kmeans.inertia_ - sse[cluster-1])\n",
    "    percent.append(delta[cluster]/sse[cluster-1])\n",
    "\n",
    "#save the results in a DataFrame\n",
    "sse_df = pd.DataFrame({'Cluster':range(1,15), 'SSE':sse[1:],'Delta':delta[1:],'Percent':percent[1:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the Elbow Curve\n",
    "plt.plot(sse_df['Cluster'], sse_df['SSE'], marker = 'o')\n",
    "plt.xticks(np.arange(0,15,step = 2))\n",
    "plt.yticks(np.arange(25000,2000000,step = 400000))\n",
    "plt.ylabel('Sum of Squared Errors', fontsize = 15)\n",
    "plt.xlabel('Cluster Number, K', fontsize = 15)\n",
    "plt.title('Elbow Curve', fontsize = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that SSE decreases as we increase k. Here, it decreases faster when k is small. As k increases, the reduction in SSE becomes smaller. We try to choose the number around the `inflection point`, where the change in SSE becomes negligible, indicating that there is little room to improve the model by increasing k. On our graph, the elbow curve becomes flat around 10.\n",
    "\n",
    "We can also look at the specific changes in SSE. In the previous step, we saved the SSE, the change in SSE (Delta), and \n",
    "the percentage change in SSE (percent) in `sse_df`.\n",
    "\n",
    "> Using the elbow curve can be entirely subjective. For instance, here, you can have a reasonable argument for 8 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the DataFrame. Do you have the same conclusion?\n",
    "sse_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint 1: Run a K-Means clustering model </h3></font> \n",
    "\n",
    "1. Take a look at the elbow curve and `sse_df`, which number(s) do you think is (are) optimal?\n",
    "\n",
    "2. Choose a cluster number that you think is best (other than 3,7, and 10). Use `Kmeans(n_cluster = , random_state = )` to run a k-means clustering model with the number you choose. Save your results and features in `frame_k`.\n",
    "\n",
    "3. Create the +/- employer characteristics table `ck_emp_charcs` with the function `characteristics_table(cols,frame_k)`.\n",
    "\n",
    "4. Compare your results with the results we got previously. Do you find any differences? Are the results improved?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Employer Characteristics Within Each Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will run the model with what we've determined to be the optimal cluster number, 10, and answer the following questions:\n",
    "- What are the characteristics of employers within each cluster?\n",
    "- What are the most common industries in each cluster?\n",
    "- Which cluster(s) of employers are \"good\" employers? Which cluster(s) of employers are \"bad\" employers?\n",
    "\n",
    "Let's run the 10-cluster model and save all the features and predicted clusters in `frame_10`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "kmeans = KMeans(n_clusters = 10, random_state = 42)\n",
    "\n",
    "# Fit the model\n",
    "kmeans.fit(scale(emp_ml))\n",
    "\n",
    "# Predict which cluster each employer belongs to\n",
    "y_means = kmeans.predict(scale(emp_ml))\n",
    "\n",
    "# Put employer IDs, industry codes, and the eleven features into frame_10\n",
    "frame_10 = pd.DataFrame(emp)\n",
    "\n",
    "# Add the predicted clusters into frame_10 as well\n",
    "frame_10['cluster'] = y_means\n",
    "\n",
    "# Let's see how many employers are in each cluster\n",
    "frame_10['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all features to a list\n",
    "cols = emp_ml.columns.tolist()\n",
    "\n",
    "# Run the function and save the results of the function to a DataFrame\n",
    "c10_emp_charcs = characteristics_table(cols,frame_10)\n",
    "\n",
    "# Let's see how the table looks like!\n",
    "c10_emp_charcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use .groupby and .agg to look at means and standard deviation of\n",
    "# employer characteristics within each cluster\n",
    "c10_emp_desc = frame_10.groupby(['cluster'])[cols].agg(['mean','std']).T.round(1)\n",
    "\n",
    "c10_emp_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAICS codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we did not include industry codes in the clustering model because categorical variables are not suitable for k-means clustering. However, now that we have ran the model, we can use industries to help describe the characteristics of employers in each cluster. Here, we will use the 2-digit industry code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert 3-digit industry codes to 2-digit\n",
    "frame_10['naics_2_digit'] = frame_10['naics_3_digit'].str[0:2]\n",
    "\n",
    "#Check what we get\n",
    "frame_10['naics_2_digit'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get their descriptions from the lookup table `oh_naics2_codes_lkp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get naics code lookup table\n",
    "# note that NAICS lookup code is numeric. We need to convert it to string by using astype(str)\n",
    "naics_lkp = pd.read_sql('select * from data_ohio_olda_2018.oh_naics2_codes_lkp',conn).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values of 2-digit NAICS code in the lookup table\n",
    "naics_lkp.naics2.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of the 2-digit NAICS codes are not in the lookup table. This is because some sectors are represented by a range of 2-digit codes. For example, manufacturing are 31-33, retail trade are 44-45, and transporation and warehousing are 48-49. Our lookup table only has one code for each of the three sectors. Therefore, we need to make some changes to the 2-digit NAICS codes in `frame_10` so they'll match the NAICS codes in the lookup table, which we can do using the `replace()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 2 digit NAICS code\n",
    "frame_10.replace(to_replace={\"naics_2_digit\": {'32': '31', '33':'31', '45':'44', '49':'48'}},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with the lookup table to get description\n",
    "frame_10 = frame_10.merge(naics_lkp, left_on='naics_2_digit',right_on='naics2', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm our table looks right\n",
    "frame_10[['naics_3_digit','naics_2_digit','naics2','naics2_desc']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most popular NAICS by cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the NAICS codes that are most present by industry!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by cluster and 2-digit industry\n",
    "c10_ind_counts = frame_10.groupby(['cluster','naics_2_digit','naics2_desc'])['employer'].agg(['count']).reset_index()\n",
    "\n",
    "# see c10_ind_counts\n",
    "c10_ind_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort values from the highest to the lowest within each cluster\n",
    "c10_ind_counts = c10_ind_counts.sort_values(by=['cluster','count'], ascending=False)\n",
    "\n",
    "#see c10_ind_counts\n",
    "c10_ind_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the top 3 industries within each cluster\n",
    "c10_top3_ind_counts = c10_ind_counts.groupby(['cluster']).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make the table a bit more readable, we will add a `rank` column to differentiate between the most popular industries by cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a rank column\n",
    "c10_top3_ind_counts['rank'] = c10_top3_ind_counts.groupby(['cluster']).cumcount()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column names so it is easier for your audience to understand\n",
    "c10_top3_ind_counts = c10_top3_ind_counts.rename(columns = {'naics_2_digit':'ind_code',\n",
    "                                                            'naics2_desc':'ind_desc',})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c10_top3_ind_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine `c10_top3_ind_counts` with our employer characteristics table, `c10_emp_charcs` to get a grasp of the different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use a loop to combine c10_emp_charcs with the top three industries \n",
    "\n",
    "for n in [1,2,3]:\n",
    "    c10_emp_charcs = c10_emp_charcs.merge(c10_top3_ind_counts[c10_top3_ind_counts['rank']==n], \n",
    "                                          on = 'cluster', how = 'outer')\n",
    "    \n",
    "    c10_emp_charcs = c10_emp_charcs.rename(columns = {'ind_code': 'ind_code_top{}'.format(n),\n",
    "                                                      'ind_desc': 'ind_desc_top{}'.format(n), \n",
    "                                                      'count':'ind_count_top{}'.format(n)})\n",
    "\n",
    "c10_emp_charcs = c10_emp_charcs.drop(['rank_x','rank_y','rank'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c10_emp_charcs.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 3 industries help us learn more about the employers in each cluster. For example, from the characteristics table `c10_emp_charcs`, we know that clusters 8 and 9 are both big firms with many high-wage and stable employees. But the top 3 industries in clusters 8 and 9 are somewhat different, which could be even more significant if we analyze 3-digit NAICS codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see more details about employers in each cluster, we can also use boxplots to visualize employer characteristics within each cluster. We have created the function `get_boxplot_stats` for you. Due to privacy and confidentiality concerns, we cannot export normal boxplots from ADRF. Instead, we need to use a safe boxplot, which has fuzzy min, max, median, etc. The function `get_boxplot_stats` returns the statistics you need for the boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxplot_stats(frame,var,k):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns stats and xticklabels for the safe boxplot.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    frame: the DataFrame that contains the employer characteristics and the predicted cluster \n",
    "    of each employer\n",
    "    var : the variable you want to plot\n",
    "    k: the list of clusters you want to plot\n",
    "    \"\"\"\n",
    "    \n",
    "    fuzzy_25 = (frame.groupby(['cluster'])[var].quantile(.2) + frame.groupby(['cluster'])[var].quantile(.3))/2\n",
    "    fuzzy_med = (frame.groupby(['cluster'])[var].quantile(.45) + frame.groupby(['cluster'])[var].quantile(.55))/2\n",
    "    fuzzy_75 = (frame.groupby(['cluster'])[var].quantile(.7) + frame.groupby(['cluster'])[var].quantile(.8))/2\n",
    "   \n",
    "    outlier_min = fuzzy_25 - 1.5*(fuzzy_75-fuzzy_25)\n",
    "    outlier_max = fuzzy_75 + 1.5*(fuzzy_75-fuzzy_25)\n",
    "    \n",
    "    df_min = pd.DataFrame([0])\n",
    "    df_max = pd.DataFrame([0])\n",
    "    stats = []\n",
    "    xticklabel = []\n",
    "    \n",
    "    for n in k:\n",
    "        df = frame[(frame['cluster']==n) & (frame[var] > outlier_min[n]) & \n",
    "                   (frame_3[var] < outlier_max[n])][var].unique()\n",
    "\n",
    "        if df.size == 0:\n",
    "            df_min['c{}'.format(n)] = fuzzy_25\n",
    "            df_max['c{}'.format(n)] = fuzzy_75\n",
    "        else:\n",
    "            df_min['c{}'.format(n)] = (sorted(df)[0] + sorted(df)[1])/2\n",
    "            df_max['c{}'.format(n)]= (sorted(df)[-1] + sorted(df)[-2])/2\n",
    "\n",
    "        stats.append ({'med': fuzzy_med[n], 'q1': fuzzy_25[n], \n",
    "                                'q3': fuzzy_75[n], 'whislo': df_min['c{}'.format(n)][0], \n",
    "                                'whishi': df_max['c{}'.format(n)][0]})\n",
    "        xticklabel.append(n)\n",
    "    \n",
    "    return stats,xticklabel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a list to save the cluster you want to look at\n",
    "cluster_to_viz = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "# Get stats and clusters from the get_boxplot_stats() function\n",
    "stats, xticklabel = get_boxplot_stats(frame_10,'num_employed',cluster_to_viz)\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.bxp(stats, showfliers=False)\n",
    "ax.set_xticklabels(xticklabel)\n",
    "\n",
    "#set the title\n",
    "ax.set_title('Number of employees during 2013Q3\\nby Cluster')\n",
    "\n",
    "#Create axes same as before\n",
    "ax.set_xlabel(\"Cluster\")\n",
    "ax.set_ylabel(\"Number of employees\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the numerical summary table, we can get the same conclusion that clusters 3,8,and 9 consist of big companies, but we would not have been able to see that cluster 3's employers have especially high number of employees. Companies in cluster 9 are bigger than companies in cluster 8 on average. How about other measures, such as earnings per employee?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a list to save the cluster you want to look at\n",
    "cluster_to_viz = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "# Get stats and clusters from the get_boxplot_stats() function\n",
    "stats, xticklabel = get_boxplot_stats(frame_10,'avg_earnings',cluster_to_viz)\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.bxp(stats, showfliers=False)\n",
    "ax.set_xticklabels(xticklabel)\n",
    "\n",
    "#set the title\n",
    "ax.set_title('Average earnings per employee \\nby Cluster')\n",
    "\n",
    "#Create axes same as before\n",
    "ax.set_xlabel(\"Cluster\")\n",
    "ax.set_ylabel(\"Average earnings\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern of average earnings are very different from that of number of employees. Employees of cluster 5 firms mostly earn extremely high quarterly incomes. While employers in clusters 8 and 9 pay their employees more on average than employers in clusters 1,2,4, and 7, they also have some low-income workers. We can further analyze earning characteristics, particularly for those more relevant to entry-level job applicants from community colleges, by looking at the bottom 25 percentile earnings of employers in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a list to save the cluster you want to look at\n",
    "cluster_to_viz = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "# Get stats and clusters from the get_boxplot_stats() function\n",
    "stats,xticklabel = get_boxplot_stats(frame_10,'bottom_25_pctile',cluster_to_viz)\n",
    "_, ax = plt.subplots()\n",
    "ax.bxp(stats, showfliers=False)\n",
    "ax.set_xticklabels(xticklabel)\n",
    "\n",
    "#set the title\n",
    "ax.set_title('Bottom 25 Percentile Earnings by Cluster')\n",
    "\n",
    "#Create axes same as before\n",
    "ax.set_xlabel(\"Cluster\")\n",
    "ax.set_ylabel(\"Bottom 25 Percentile Earnings\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do high average earnings imply that an employer is \"good\"? Not necessarily. An employer could have a few high-wage management level workers and many low-wage workers but still ends up with high average earnings. The bottom 25 percentile earnings allow us to have a deeper understanding about the workers that have lower incomes within a firm. For example, the average earnings of clusters 2 and 4 are about the same, but the bottom 25 percentile earnings of cluster 2 employers are lower, implying employers in cluster 2 are likely to be of a lower quality pay-wise than that of cluster 4 employers. \n",
    "\n",
    "> We can do the same practice for the other employer-level measures. Just keep in mind, if you want to export these boxplots, **please drop clusters that have less than 10 employers**. This will be discussed further in the [Disclosure Review](03_Disclosure_Review.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint 2: Examine employer characteristics of your model </h3></font> \n",
    "\n",
    "1. Given the `frame_k` you have created in checkpoint 1, find out the top 3 industries within each cluster. You can use 3-digit or 2-digit industry codes.\n",
    "\n",
    "2. Create boxplots for `avg_earnings`, `full_quarter_avg_earnings`, and `top_25_pctile`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. How are 2012-13 Ohio Community College Graduates distributed amongst our clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should have a decent grasp of Ohio's employers during 2013Q3. Next, we will focus on our cohort of interest, 2012-13 Ohio community college students, and their employers by using our previous clustering results. Specifically, in this section, we will explore:\n",
    "\n",
    "- How are the 2012-13 Ohio community college graduates distributed amongst the 10 clusters?\n",
    "- How are the employers of 2012-13 Ohio community college graduates distributed amongst the 10 clusters?\n",
    "- What are average earnings (from primary employers) of graduates in each cluster?\n",
    "- What are the top 3 industries of graduates in each cluster?\n",
    "- What are the top 3 degrees of graduates in each cluster?\n",
    "\n",
    "Recall that in the [Data Exploration](02_1_Data_Exploration.ipynb) notebook, we joined our cohort table with `oh_ui_wage_by_employer` to create `cohort_oh_jobs_emp`. This table contains employment records of our cohort during the first year after graduation. For those with multiple employers during a quarter, we have only kept the employers that paid them the most during that quarter. \n",
    "\n",
    "Let's pull that table from database and limit it to records from the third quarter of 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Cohort in 2013Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull 2012-13 Ohio Community College graduates and the employer that paid them the most\n",
    "qry = '''\n",
    "select * from ada_20_osu.cohort_oh_jobs_emp\n",
    "where job_date='2013-07-01';\n",
    "'''\n",
    "\n",
    "cc_emp_df = pd.read_sql(qry,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see cc_emp_df\n",
    "cc_emp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many people in our cohort were employed during 2013 Q3.\n",
    "cc_emp_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's merge `cc_emp_df` with `frame_10` based on each person's primary employers and see how our cohort and their employers distribute across clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge tables and only keep the variables that we will use for analysis \n",
    "cc_df = frame_10[['employer','cluster']].merge(cc_emp_df[['ssn_hash','employer','wages','naics_3_digit']], \n",
    "                                               on = 'employer', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see cc_df\n",
    "cc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohort's Earnings Outcomes by Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can see where our cohort landed and if those in our cohort experiencing similar earnings outcomes across the clusters to the general Ohio workforce population in 2013Q3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2012-13 Community college graduates' distribution\n",
    "cc_cluster = cc_df.groupby(['cluster'])['ssn_hash'].nunique().reset_index()\n",
    "\n",
    "cc_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2012-13 community college graduates' employers' distribution\n",
    "cc_emp_df = cc_df.groupby(['cluster'])['employer'].nunique().reset_index()\n",
    "\n",
    "# see cc_emp_df\n",
    "cc_emp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of employers within each cluster\n",
    "c10_counts = get_counts(frame_10).rename(columns = {'n':'cluster_n'})\n",
    "\n",
    "# see c10_counts\n",
    "c10_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the three DataFrames\n",
    "cc_cluster = cc_cluster.merge(cc_emp_df, on = 'cluster')\n",
    "cc_cluster = cc_cluster.merge(c10_counts, on = 'cluster', how = 'outer')\n",
    "\n",
    "#rename columns to be more descriptive\n",
    "cc_cluster = cc_cluster.rename(columns = {'ssn_hash':'stud_n','employer':'emp_n'})\n",
    "\n",
    "# see cc_cluster\n",
    "cc_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add more columns to this table. Specifically, we will calculate:\n",
    "\n",
    "1. The percentage of employers in (1) that are in each cluster `emp_per`\n",
    "2. The percentage of 2012-13 Ohio community college graduates in each cluster `stud_per`\n",
    "3. The average earnings (from primary employers) of graduates in each cluster `avg_earnings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percent of employers in each cluster\n",
    "cc_cluster['emp_per'] = ((cc_cluster['emp_n'] / cc_cluster['emp_n'].sum())*100).round(2).astype(str) + '%'\n",
    "\n",
    "# Percent of graduates in each cluster\n",
    "cc_cluster['stud_per'] = ((cc_cluster['stud_n'] / cc_cluster['stud_n'].sum())*100).round(2).astype(str) + '%'\n",
    "\n",
    "#get the average earnings\n",
    "avg_earn = cc_df.groupby(['cluster'])['wages'].agg(['mean']).reset_index()\n",
    "\n",
    "cc_cluster = cc_cluster.merge(avg_earn, on = 'cluster', how = 'outer')\n",
    "cc_cluster = cc_cluster.rename(columns = {'mean':'avg_earnings'})\n",
    "\n",
    "cc_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that REDACTED of these graduates that were employed in 2013Q3 are hired by employers in the largest cluster, cluster 7. Their employers account for REDACTED of the employers that hired at least one member of our cohort. The average earnings of graduates in this cluster are the third lowest among all the clusters at REDACTED dollars per quarter, which translates to REDACTED dollars per year. Based on the graphs and tables we generated previously, cluster 7 consists of primarily small employers that pay their employees low wages. However, job turnover rates for these employers are relatively low when compared with employers in other clusters. \n",
    "\n",
    "The second and third most popular clusters for our cohort clusters 8 and 9, which contain fewer employers than all the other clusters, except for cluster 3. The two clusters account for only REDACTED of the employers that hired someone in our cohort, but they hired REDACTED of those in our cohort employed in 2013Q3. However, the average earnings of graduates in these two clusters are not much higher than graduates in cluster 7. This is consistent with the average earnings and bottom 25 percentile earnings boxplots.\n",
    "\n",
    "Graduates in clusters 1 and 5 have the highest average earnings. Again, this is consistent with what the boxplots show: the average earnings and bottom 25 percentile earnings are the highest in clusters 1 and 5. However, these graduates make up less than REDACTED\n",
    "of our cohort that found employment in 2013Q3.\n",
    "\n",
    "> Note that the average earnings in this table are average earnings from the primary employers. These may understate some people's earnings because some of them may have worked for more than one employers during 2013Q3. However, if they have multiple jobs and each job does not pay them much, this does not affect the conclusion that some of them were working for low-quality employers after graduation.\n",
    "\n",
    "> One fact our model cannot capture is that some community college graduates may have gone to a four-year college during the time period we are looking at. Thus, they are either not in the labor market or have part-time jobs which pay them little and may not match their skillset. \n",
    "\n",
    "> Note that no members of our cohort were hired by employers in cluster 6. Why do you think this may be the case? Refer back to the employers table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohort-specific NAICS by cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can find the top 3 industries for our cohort by cluster. To do so, we need to repeat the steps we did previously: 1) convert three-digit industry codes to two-digits; 2) make changes to the special two-digit industry codes; 3) join it with the lookup table to get indsutry description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 3-digit NAICS codes to 2-digit NAICS codes\n",
    "cc_df['naics_3_digit'] = cc_df['naics_3_digit'].fillna(\"missing\").astype(str)\n",
    "cc_df['naics_2_digit'] = cc_df['naics_3_digit'].str[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the codes in Manufacturing, Retail Trades, and Transportation and Warehouse\n",
    "cc_df.replace(to_replace={\"naics_2_digit\": {'32': '31', '33':'31', '45':'44', '49':'48'}},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the NAICS codes' descriptions from the lookup table\n",
    "cc_df = cc_df.merge(naics_lkp, left_on='naics_2_digit',right_on='naics2',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by cluster and 2-digit NAICS codes\n",
    "cc_top3_ind = cc_df.groupby(['cluster','naics2','naics2_desc'])['ssn_hash'].agg(['count']).reset_index()\n",
    "\n",
    "# Sort counts from the highest to the lowest within each cluster\n",
    "cc_top3_ind = cc_top3_ind.sort_values(by=['cluster','count'], ascending=False)\n",
    "\n",
    "# Get the top 3 industries\n",
    "cc_top3_ind = cc_top3_ind.groupby(['cluster']).head(3)\n",
    "\n",
    "# Add the rank so it is easier for us to see in the table\n",
    "cc_top3_ind['rank'] = cc_top3_ind.groupby(['cluster']).cumcount()+1\n",
    "\n",
    "cc_top3_ind.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine our main table with the top three industries \n",
    "\n",
    "for n in [1,2,3]:\n",
    "    cc_cluster = cc_cluster.merge(cc_top3_ind[cc_top3_ind['rank']==n], on = 'cluster', how = 'outer')\n",
    "    cc_cluster = cc_cluster.rename(columns = {'naics2': 'naics2_top{}'.format(n),\n",
    "                                              'naics2_desc': 'naics2_desc_top{}'.format(n), \n",
    "                                              'count':'ind_count_top{}'.format(n)})\n",
    "\n",
    "cc_cluster = cc_cluster.drop(['rank_x','rank_y','rank'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohort's Most Popular Degrees by Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also see the most popular degrees by cluster. To do so, we need to recreate `cc_grads_recent`, which has the degree records of 2012-13 Ohio community college students and is limited to the most recent degree for students who have earned multiple degrees during 2012-13 academic year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 2012-13 Ohio community college graduates\n",
    "\n",
    "qry = '''\n",
    "drop table if exists cc_grads;\n",
    "create temp table cc_grads as\n",
    "select a.*, lkp.*\n",
    "from data_ohio_olda_2018.oh_hei_long a\n",
    "left join data_ohio_olda_2018.oh_hei_campus_county_lkp lkp\n",
    "on a.degcert_campus = lkp.campus_num\n",
    "where ((a.degcert_yr_earned = '2012' and (a.degcert_term_earned = '4' or a.degcert_term_earned = '1')) or \n",
    "    (a.degcert_yr_earned = '2013' and (a.degcert_term_earned = '2' or a.degcert_term_earned = '3'))) and \n",
    "    lkp.campus_type_code in ('TC', 'SC', 'CC') and a.degcert_subject!='TRAMOD';\n",
    "'''\n",
    "conn.execute(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the most recent degree for students who have earned more than one degrees\n",
    "qry = '''\n",
    "drop table if exists cc_grads_recent;\n",
    "create temp table cc_grads_recent as\n",
    "select distinct on (ssn_hash) *, left(degcert_subject, 2) as subject\n",
    "from (\n",
    "SELECT *, \n",
    "    CASE WHEN degcert_term_earned = 4 THEN\n",
    "        format('%%s-%%s-01', degcert_yr_earned, 7)::date \n",
    "    WHEN degcert_term_earned = 1 THEN\n",
    "        format('%%s-%%s-01', degcert_yr_earned, 10)::date \n",
    "    WHEN degcert_term_earned = 2 THEN\n",
    "        format('%%s-%%s-01', degcert_yr_earned, 1)::date \n",
    "    WHEN degcert_term_earned = 3 THEN\n",
    "        format('%%s-%%s-01', degcert_yr_earned, 4)::date \n",
    "    END AS deg_date\n",
    "    from cc_grads\n",
    ") q\n",
    "order by ssn_hash, deg_date DESC\n",
    "'''\n",
    "conn.execute(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to convert 6-digit subject codes to 2-digit ones to get their descriptions from the lookup table `oh_subject_codes_lkp_new`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get degrees and degree descriptions\n",
    "qry = '''\n",
    "with cc_subject as (select ssn_hash, left(right('0'||degcert_subject,6),2) from cc_grads_recent)\n",
    "select * from cc_subject\n",
    "left join (select distinct(subject_code_2010_2),subject_desc_2010_2\n",
    "      from data_ohio_olda_2018.oh_subject_codes_lkp_new) lkp\n",
    "on cc_subject.left=lkp.subject_code_2010_2; \n",
    "'''\n",
    "\n",
    "cc_subject = pd.read_sql(qry,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_subject.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's add the degree information to cc_df\n",
    "cc_df = cc_df.merge(cc_subject, on = 'ssn_hash', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's find out the top 3 degrees for students in each cluster and add it to `cc_cluster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by cluster and subject codes to get counts\n",
    "cc_top3_cip = cc_df.groupby(['cluster','subject_code_2010_2','subject_desc_2010_2'])['ssn_hash'].agg(['count']).reset_index()\n",
    "\n",
    "# see cc_top3_cip\n",
    "cc_top3_cip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the counts from highest to lowest within each cluster\n",
    "cc_top3_cip = cc_top3_cip.sort_values(by=['cluster','count'], ascending=False)\n",
    "\n",
    "# see cc_top3_cip\n",
    "cc_top3_cip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 3 degrees with in each cluster\n",
    "cc_top3_cip = cc_top3_cip.groupby(['cluster']).head(3)\n",
    "\n",
    "# see cc_top3_cip\n",
    "cc_top3_cip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a rank column\n",
    "cc_top3_cip['rank'] = cc_top3_cip.groupby(['cluster']).cumcount()+1\n",
    "\n",
    "# see cc_top3_cip\n",
    "cc_top3_cip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percent of students with the top 3 degrees within each cluster\n",
    "cc_top3_cip['degree_per'] = ((cc_top3_cip['count']/(cc_cluster['stud_n'].sum()))*100).round(2).astype(str) + '%'\n",
    "\n",
    "# see cc_top3_cip\n",
    "cc_top3_cip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_top3_cip = cc_top3_cip.rename(columns = {'subject_code_2010_2':'degree_code',\n",
    "                                            'subject_desc_2010_2':'degree_desc',})\n",
    "\n",
    "# see cc_top3_cip\n",
    "cc_top3_cip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine our main table with the top three degrees \n",
    "\n",
    "for n in [1,2,3]:\n",
    "    cc_cluster = cc_cluster.merge(cc_top3_cip[cc_top3_cip['rank']==n], on = 'cluster', how = 'outer')\n",
    "    cc_cluster = cc_cluster.rename(columns = {'degree_code': 'degree_code_top{}'.format(n),\n",
    "                                              'degree_desc': 'degree_desc_top{}'.format(n), \n",
    "                                              'count':'degree_count_top{}'.format(n),\n",
    "                                              'degree_per':'degree_per_top{}'.format(n)})\n",
    "\n",
    "cc_cluster = cc_cluster.drop(['rank_x','rank_y','rank'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_cluster.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a closer look at our cohort and the clusters again. \n",
    "\n",
    "The top 3 industries and the top 3 degrees in each cluster are dominated by the REDACTED industry and graduates with degrees in REDACTED. This is not suprising. Recall that in the [Data Exploration](01_2_Dataset_Exploration.ipynb) notebook, we found that the degree with the most graduates is REDACTED. Some of them ended up in \"high-quality\" employers clusters (such as cluster 1), but others ended up in \"low-quality\" employers cluster (such as cluster 7). This may be related to the variations of REDACTED workers' skill levels and entry level earnings. For example, according to Bureau of Labor Statistics, the median annual wage of REDACTED is only REDACTED, while a REDACTED can expect to earn REDACTED at an entry-level position.\n",
    "\n",
    "Similarly, when we look at graduates in clusters 7,8,and 9, the top 3 degrees are the same: REDACTED, REDACTED, and REDACTED. However, cluster 7 graduates are more likely to find employment in industries with many low-skill and low-income jobs, while some of the cluster 9 graduates work at more profitable industries, such as REDACTED.\n",
    "\n",
    "Among graduates in the two high-quality employer clusters, REDACTED are amongst the Top 3 degrees. This is consistent with the high expected earnings of people with REDACTED degrees.\n",
    "\n",
    "In conclusion, in our model, graduates in some degrees are more spread out across clusters, especially those in REDACTED, while students with other degrees, such as REDACTED, are more likely to find jobs with \"high-quality\" employers. Note that we have only used 2-digit industry codes and 2-digit subject codes. We may have a better understanding of our cohorts' employment if we use more detailed industry codes and subject codes. However, you need to make sure that the number of people and number of employers in each subgroup are more than 10 if you would like to export these results outside of ADRF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint 3: Find out labor market and education characteristics for 2012-13 community college graduates </h3></font> \n",
    "\n",
    "1. Given the `frame_k` you have created in checkpoint 1, find out how many 2012-13 community college students are in each cluster of your model.\n",
    "\n",
    "2. What are the average earnings (from primary employers) for graduates in each cluster of your model?\n",
    "\n",
    "3. What are the top 3 degrees of graduates in each cluster of your model? You can use 2-digit or 4-digit subject codes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Davis, Steven J., and John Haltiwanger. \"Gross job creation, gross job destruction, and employment reallocation.\" *The Quarterly Journal of Economics* 107, no. 3 (1992): 819-863."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foster, Ian, Rayid Ghani, Ron S. Jarmin, Frauke Kreuter, and Julia Lane, eds. *Big data and social science: A practical guide to methods and tools.* crc Press, 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elka, Torpey. \"Employment outlook for occupations requiring an associate's degree, certificate or some college.\" *Career Outlook*, U.S. Bereau of Labor Statistics, November 2018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-ada",
   "language": "python",
   "name": "py3-ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
