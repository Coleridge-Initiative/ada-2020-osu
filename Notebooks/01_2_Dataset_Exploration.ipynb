{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"images/CI_horizontal.png\" width=\"600\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n",
    "\n",
    "Rayid Ghani, Frauke Kreuter, Julia Lane, Brian Kim, Adrianne Bradford, Alex Engler, Nicolas Guetta Jeanrenaud, Graham Henke, Daniela Hochfellner, Clayton Hunter, Avishek Kumar, Jonathan Morgan, Ursula Kaczmarek, Benjamin Feder, Ekaterina Levitskaya, Lina Osorio-Copete, Tian Lou."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In an ideal world, we would have all of the data we want with all of the desirable properties (no missing values, no errors, standard formats, and so on). We'd also have perfect data documentation, with summary statistics and approproiate aggregate measures of everything we'd want to investigate. However, that is hardly ever true - so we have to work with using our datasets to answer questions of interest as intelligently as possible. \n",
    "\n",
    "In this notebook, we will discover the datasets we have on the ADRF and we will use our datasets to answer some questions of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "This notebook will give you the opportunity to spend some hands-on time with the data. \n",
    "\n",
    "Throughout the notebook, you will work through various techniques of how to use SQL and Python to explore the various datasets in the ADRF and better understand what you are working with. This will form the basis of all the other types of analyses you will do in this class and is a crucial first step for any data analysis workflow. As you work through the notebook, we will have checkpoints for you try out your own code, but you can also think about how you might apply any of the techniques and code presented with other datasets as well. \n",
    "\n",
    "We are going to show just a portion of what you might be interested in investigating, so don't feel restricted by the questions we've decided to try to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datasets We Will Explore In This Notebook:**\n",
    "- **Ohio Higher Education Information (HEI) data**: Ohio public college student information (student enrollment, degree earned, demographic, course credits, institution).\n",
    "- **Ohio Unemployment Insurance (UI) Wage data**: Ohio workers' quarterly earnings and employment.\n",
    "- **Indiana Unemployment Insurance (UI) Wage data**: Indiana workers' quarterly earnings and employment. \n",
    "\n",
    "You will explore these datasets using both SQL and `pandas` in Python. The `sqlalchemy` Python package will give you the opportunity to interact with the database using SQL to pull data into Python. Some additional manipulations will be handled by Pandas in Python (by converting your datasets into dataframes). We've also provided a [supplementary notebook](01_2_Dataset_Exploration_supplemental.ipynb) for you that walks you through how to do these same analyses on the OTC data.\n",
    "\n",
    "**This notebook will provide an introduction and examples for:**\n",
    "\n",
    "- How to create new tables from the larger tables in database (sometimes called the \"analytical frame\")\n",
    "- How to explore different variables of interest\n",
    "- How to create aggregate metrics\n",
    "- How to join tables\n",
    "- How to generate descriptive statistics to describe a specific cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "\n",
    "We will be using the `sqlalchemy` Python package to access tables in our class database server - PostgreSQL. \n",
    "\n",
    "To read the results of our queries, we will be using the `pandas` Python package, which has the ability to read tabular data from SQL queries into a pandas DataFrame object. Within `pandas`, we will use various commands:\n",
    "\n",
    "- `fillna`\n",
    "- `groupby`\n",
    "- `nunique`\n",
    "\n",
    "Within SQL, we will use various queries to:\n",
    "\n",
    "- select data subsets\n",
    "- sum over groups\n",
    "- create new tables\n",
    "- count distinct values of desired variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "\n",
    "In Python, we `import` packages. The `import` command allows us to use libraries created by others in our own work by \"importing\" them. You can think of importing a library as opening up a toolbox and pulling out a specific tool. Among the most famous Python packages:\n",
    "- **numpy** is short for \"numerical Python\". `numpy` is a lynchpin in Python's scientific computing stack. Its strengths include a powerful *N*-dimensional array object, and a large suite of functions for doing numerical computing. \n",
    "- **pandas** is a library in Python for data analysis that uses the DataFrame object (modeled after R DataFrames, for those familiar with that language) which is similiar to a spreedsheet but allows you to do your analysis programaticaly rather than the point-and-click of Excel. It is a lynchpin of the PyData stack and is built on top of `numpy`.  \n",
    "- **sqlalchemy** is a Python library for interfacing with a PostGreSQL database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas-related imports\n",
    "import pandas as pd\n",
    "\n",
    "# Numpy\n",
    "import numpy as np\n",
    "\n",
    "# database interaction imports\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When in doubt, use shift + tab to read the documentation of a method by placing a cursor near the name of the method and pressing shift_+tab.__\n",
    "\n",
    "__The `help()` function provides information on what you can do with a function.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example\n",
    "help(pd.read_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "We can execute SQL queries using Python to get the best of both worlds. For example, Python - and `pandas` in particular - makes it much easier to calculate descriptive statistics and perform more complicated analyses with the data. Additionally, as we will see in the Data Visualization exercises, it is relatively easy to create data visualizations using Python. \n",
    "\n",
    "Pandas provides many ways to load data. It allows the user to read the data from a local csv or excel file, pull the data from a relational database, or read directly from a URL (when you have internet access). Since we are working with the PostgreSQL database `appliedda` in this course, we will demonstrate how to use pandas to read data from a relational database. For examples on reading data from a CSV file, refer to the pandas documentation [Getting Data In/Out](pandas.pydata.org/pandas-docs/stable/10min.html#getting-data-in-out).\n",
    "\n",
    "The function to run a SQL query and pull the data into a pandas dataframe (more to come) is `pd.read_sql()`. Just like doing a SQL query from DBeaver, this function will ask for some information about the database, and what query you would like to run. Let's walk through the example below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish a Connection to the Database\n",
    "\n",
    "The first parameter is the connection to the database. To create a connection we will use the SQLAlchemy package and tell it which database we want to connect to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Database Connection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to create a connection to the database, \n",
    "# we need to pass the name of the database and host of the database\n",
    "\n",
    "host = 'stuffed.adrf.info'\n",
    "DB = 'appliedda'\n",
    "\n",
    "connection_string = \"postgresql://{}/{}\".format(host, DB)\n",
    "conn = sqlalchemy.create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note we can parameterize Python `string` objects - using the built-in `.format()` function. We will use various formulations in the program notebooks (e,g. when building queries), some examples are:\n",
    "1. Empty brackets (shown above) which simply inserts the variable in the string; when there is more than one set of brackets Python will insert variables in the order they are listed\n",
    "2. Brackets with formatting can be used to make print statements more readable (eg `'text with formatted number with comma and 1-digit decimal {:,.1f}'.format(number_value)` will print `123,456.7` instead of `123456.7123401`)\n",
    "3. Named brackets to use the same variables multiple times in a text block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulate Data Query\n",
    "\n",
    "This part is similar to writing a SQL query in DBeaver. Depending on the data we are interested in, we can use different queries to pull different data. In this example, we will pull in academic content from the HEI data for post-secondary education attendees in the 2013 calendar year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__create a query as a `string` object in Python__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT *\n",
    "FROM data_ohio_olda_2018.oh_hei_long \n",
    "WHERE file_year = '2013'\n",
    "LIMIT 5\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The three quotation marks surrounding the query body is called multi-line string. It is quite handy for writing SQL queries because the new line character will be considered part of the string, instead of breaking the string.\n",
    "\n",
    "> Note that `file_year` is the year the data was reported. In the later section, we will learn how to identify the school year a student enrolled in/graduated from an institution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have defined a variable `query`, we can call it in the code\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that the `LIMIT` provides one simple way to get a \"sample\" of data; however, using `LIMIT` does **not provide a _random_** sample. You may get different samples of data than others using just the `LIMIT` clause, but it is just based on what is fastest for the database to return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Data from the Database\n",
    "\n",
    "Now that we have the two parameters (database connection and query), we can pass them to the `pd.read_sql()` function to obtain the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we pass the query and the connection to the pd.read_sql() function and assign the variable `df`\n",
    "# to the dataframe returned by the function\n",
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint 1: Read in the HEI table </h3></font> \n",
    "\n",
    "Read in and explore a subsample of the HEI long table (table name: `oh_hei_long`). \n",
    "\n",
    "Try to:\n",
    "1. limit the sample to 20 observations\n",
    "2. look at records of people enrolled in 2012 (e.g., `enroll_yr_num`='2012')\n",
    "3. count the number of people enrolled in 2012 (e.g., use `count(distinct(ssn_hash))` in your query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query= '''\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Using Python and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is in the Database?\n",
    "\n",
    "There are a few different ways to connect and explore the data in the database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Schemas, Tables, and Columns in database__\n",
    "\n",
    "Let's pull the list of schema names in the database, the list of tables in these schemas, and the list of columns in these tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See all available schemas:\n",
    "query = '''\n",
    "SELECT schema_name \n",
    "FROM information_schema.schemata;\n",
    "'''\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, in this class you have access to the following schemas: `public`, `data_ohio_olda_2018`, `il_des_kcmo`, `kcmo_lehd`, `mo_dhe`,`in_dwd`, `in_che`, and `ada_20_osu`. You only have write access to the `ada_20_osu` schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas = \"\"\"\n",
    "'public', 'data_ohio_olda_2018', 'il_des_kcmo', 'kcmo_lehd', 'mo_dhe', 'in_dwd', 'in_data_2019', 'ada_20_osu'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm our schemas exist with \n",
    "# an updated version of the previous query\n",
    "query = '''\n",
    "SELECT schema_name \n",
    "FROM information_schema.schemata\n",
    "WHERE schema_name IN ({})\n",
    "'''.format(schemas)\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT schemaname, tablename\n",
    "FROM pg_tables\n",
    "WHERE schemaname IN ({})\n",
    "'''.format(schemas)\n",
    "\n",
    "tables = pd.read_sql(query, conn)\n",
    "# print tables not in the public schema\n",
    "print(tables.query(\"schemaname != 'public'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the tables in the OLDA schema:\n",
    "sorted(tables[tables[\"schemaname\"] == 'data_ohio_olda_2018']['tablename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note the two ways shown above to subset a `Pandas.DataFrame`:\n",
    "1. Use the built-in `.query()` function (done in this line: `tables.query(\"schemaname != 'public'\"`) )\n",
    "2. Create an array of `True` and `False` values (done in this line: `tables[\"schemaname\"] == 'data_ohio_olda_2018'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can look at column names within tables\n",
    "# here we'll set the schema and table with variables\n",
    "\n",
    "schema = 'data_ohio_olda_2018'\n",
    "tbl = 'oh_hei_long'\n",
    "\n",
    "query = '''\n",
    "SELECT * \n",
    "FROM information_schema.columns \n",
    "WHERE table_schema = '{}' AND table_name = '{}'\n",
    "'''.format(schema, tbl)\n",
    "\n",
    "# read and print results\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "In this section, you'll start looking at aggregate statistics on the data. As you work through this section, try to ask yourself some questions such as: \n",
    "- What variables are you interested in? \n",
    "- What variables do you need to identify the sample you are interested in?\n",
    "- In which table(s) are these variables available? \n",
    "- Are there any missing values in these variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> <h3> __Motivating Question # 1__: </h3> </font>\n",
    "\n",
    "Assume you are eventually interested in looking at labor market outcomes of Ohio community college students who received their degrees during the 2012-13 academic year. You will need to combine education and employment data (in this case, Ohio HEI and UI wage records). First, though, you should take some steps to better understand your cohort, so you'll first focus on these questions:\n",
    "\n",
    "**How many students got their degrees from Ohio community colleges during the 2012-13 academic year? How does the number vary by the regional location of the college and by degree field?**\n",
    "\n",
    "*JobsOhio Region:* The state of Ohio divides the state into 6 regions for economic development purposes: Southeast, Southwest, Central, West, Northwest, and Northeast Ohio.\n",
    "\n",
    "*2012-13 academic year:* According to Ohio Department of Higher Education, it is defined as the Summer and Autumn semesters of 2012 and the Winter and Spring semesters of 2013. To look at graduates' information, you can use variables that start with `degcert_`. In this case, you will need to limit the sample by using the semester (`degcert_term_earned`) and the year a person received their degree (`degcert_yr_earned`).\n",
    "\n",
    "`degcert_term_earned`:\n",
    "1= Autumn,\n",
    "2= Winter,\n",
    "3= Spring,\n",
    "4= Summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all higher education graduates in school year 2012-13\n",
    "qry = '''\n",
    "select *\n",
    "from data_ohio_olda_2018.oh_hei_long\n",
    "where (degcert_yr_earned = '2012' and (degcert_term_earned = '4' or degcert_term_earned = '1')) or \n",
    "    (degcert_yr_earned = '2013' and (degcert_term_earned = '2' or degcert_term_earned = '3'))\n",
    "'''\n",
    "df = pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be using this table to further subset to community college graduates, we will save the above SQL query in a temporary table. Temporary tables are similar to saving tables/dataframes in python, as they store a table that we can use for future reference, but it needs to be created every time you re-open or redo any analysis.\n",
    "\n",
    "In general, it is best practice to test out your queries on a small subset of a table (i.e. with a limit) before creating the temporary table, otherwise you may have to delete and recreate your temporary tables if there were any issues during your initial creation.\n",
    "\n",
    "> It is possible to perform the following merges in Python, but due to the relative sizes of the tables in question, you may run into memory issues using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store query to find 2012-13 academic year graduates in a temporary table\n",
    "# use conn.execute instead of pd.read_sql because there is no output\n",
    "qry = '''\n",
    "create temp table all_grads as\n",
    "select *\n",
    "from data_ohio_olda_2018.oh_hei_long\n",
    "where (degcert_yr_earned = '2012' and (degcert_term_earned = '4' or degcert_term_earned = '1')) or \n",
    "    (degcert_yr_earned = '2013' and (degcert_term_earned = '2' or degcert_term_earned = '3'))\n",
    "'''\n",
    "conn.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''\n",
    "select count(*) from all_grads\n",
    "'''\n",
    "pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to identify the community college graduates in our sample. You can find them by looking at the type of institutions/campuses they graduated from. There are five types of institions/campuses in the HEI data: University Main Campus (UM); University Branch Campus (UB), Community College (CC), State College (SC), Technical College (TC). You need to limit your sample to students who graduated from community colleges (**CC**), state colleges (**SC**), and technical colleges (**TC**).\n",
    "\n",
    ">  Note that in order to store information more efficiently, our main table, `oh_hei_long`, only has campus numbers (`degcert_campus`). If you want to know more details about the campus, such as campus type and campus location, you need to join the main table with the lookup table `oh_hei_campus_county_lkp`.\n",
    "\n",
    ">  In the schema `data_ohio_olda_2018`, we added suffix `_lkp` to all the lookup tables. You will need to join your main tables with lookup tables to find more detailed information about the students (such as demographics) as well as the institutions/campuses (such as county, region, and campus type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see oh_hei_campus_county_lkp table\n",
    "qry = '''\n",
    "select *\n",
    "from data_ohio_olda_2018.oh_hei_campus_county_lkp\n",
    "limit 5\n",
    "'''\n",
    "pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create temp table because this is our cohort of 2012-13 community college graduates\n",
    "qry = '''\n",
    "create temp table cc_grads as\n",
    "select a.*, lkp.*\n",
    "from all_grads a\n",
    "left join data_ohio_olda_2018.oh_hei_campus_county_lkp lkp\n",
    "on a.degcert_campus = lkp.campus_num\n",
    "where lkp.campus_type_code in ('TC', 'SC', 'CC')\n",
    "'''\n",
    "conn.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read cc_grads into python\n",
    "qry = '''\n",
    "select * from cc_grads\n",
    "'''\n",
    "df=pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of understanding your cohort, do you suspect that there might be some individuals with multiple records? Let's see if that is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the total number of records in our 2012-13 community college graduate dataframe\n",
    "df['ssn_hash'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, you just need to find the unique count of `ssn_hash` values in `df.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the number of unique key_ids (person identifiers) in our dataframe\n",
    "df['ssn_hash'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've found the answer to the first part of this motivating question. Next, you will work through breaking down the data by Ohio job region: `jobsohioregion`. You can use `.groupby()` command in Python. However, if you want to find breakdown by many demographic variables, such as students' county of residence, you cannot use your current DataFrame/temporary table since it does not contain much demographic information. You would need to go through one more step: joining `cc_grads` to the `oh_hei_demo` table in the `data_ohio_olda_2018` schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of graduates by region\n",
    "df.groupby(['jobsohioregion'])['ssn_hash'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.groupby()` method in Python ignores missing values. Thus, to make sure you don't forget to include missing values in your answer to this motivating question, you can use `fillna()` to fill missing values with something you know doesn't exist for the column(s) in question. Here, you can use -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if there are missing values in jobsohioregion\n",
    "df['jobsohioregion'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case there is any missing values, you can use fillna()\n",
    "df['jobsohioregion'].fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common practice is to look at number of graduates by fields. In Ohio's HEI data, you can use `degcert_subject` to find what field a student's degree focuses on. However, the subject code in the data is 6-digits, making it hard to differentiate between subjects. To avoid this issue, you can convert them to 2-digit codes so that you have less subject groups, making it easier to perform analyses.\n",
    "\n",
    "The following queries will help you get the 2-digit subject code. In these queries, you will try out an alternative to the temporary table in SQL using the `with` command in SQL. You can also get the description of the subject code by joining `cc_grads` with the lookup table `oh_subject_codes_lkp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['ssn_hash','degcert_subject']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first look at oh_subject_codes_lkp\n",
    "qry = '''\n",
    "select *\n",
    "from data_ohio_olda_2018.oh_subject_codes_lkp\n",
    "limit 5\n",
    "'''\n",
    "pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with creates a mini table\n",
    "# In the last line of the query, we use ::varchar to convert 'subject_code' in the lkp table from integer\n",
    "# to text. This is because when we join tables, the variable types should be the same. \n",
    "qry= '''\n",
    "with subject as (select ssn_hash, left(degcert_subject,2) as code from cc_grads)\n",
    "select subject.ssn_hash, lkp.subject_code_2010, lkp.subject_desc \n",
    "from subject\n",
    "join data_ohio_olda_2018.oh_subject_codes_lkp lkp\n",
    "on subject.code=lkp.subject_code_2010::varchar; \n",
    "'''\n",
    "\n",
    "subject_df=pd.read_sql(qry,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any missing values in subject_code?\n",
    "subject_df['subject_code_2010'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't need to worry about using fillna() and can sort using sort_values()\n",
    "subject_df.groupby(['subject_code_2010', 'subject_desc'])['ssn_hash'].nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint 2: Explore school year 2012-13 community college graduates </h3></font> \n",
    "\n",
    "How does the number vary by region and by degree field? Use `jobsohioregion` and the 2-digit subject code.\n",
    "\n",
    "For example, let's take a look at community college students that graduated in Northeast region. Which degree fields have the most community college graduates?\n",
    "\n",
    "Are there any NAs? If so, how many?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query= '''\n",
    "\n",
    "'''\n",
    "\n",
    "practice_df=pd.read_sql(query,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have completed Motivating Question #1, you will move onto Motivating Question #2, where, as promised, you will begin to explore wage records data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3>__Motivating Question # 2__:</h3></font>\n",
    "**How many 2012-13 Ohio community college graduates are employed in Ohio one year after graduation? How many of them have stable employment? How does the number vary by industry?**\n",
    "\n",
    "In this example, we will examine:\n",
    "- How many people have positive earnings during the first year after graduation?\n",
    "- What are the earning distributions within a year's time of graduates who have positive earnings during the first year after graduation?\n",
    "- How many people achieved stable employment within the first year after graduation? \n",
    "    - **Stable employment metric 1**: have positive earnings during ALL four quarters after graduation\n",
    "    - **Stable employment metric 2**: work for the same employer during the second quarter and the fourth quarter after graduation\n",
    "- How does the number of people who have stable employment vary by industry?\n",
    "\n",
    "In this example, you will join the table `cc_grads` you created in the previous question with Ohio UI wage data. In the ADRF, there are two UI wage tables for Ohio:\n",
    "- `oh_ui_wage_by_employer`: each row shows a worker's quarterly earnings, weeks worked, and industry of one employer. Note that some people may have more than one employer.\n",
    "- `oh_ui_wage_by_quarter`: each row shows a worker's total quarterly earnings, maximum weeks worked during a quarter, and total number of employers during a quarter. \n",
    "\n",
    ">*Which table should you use when you look at post-graduation labor market outcomes?*\n",
    " It depends on how you define \"employed\" and the employment metrics you use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the Unemployment Insurance wage records tables so you can get an idea of the tables' structures first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''\n",
    "select * \n",
    "from data_ohio_olda_2018.oh_ui_wage_by_employer\n",
    "limit 5\n",
    "'''\n",
    "\n",
    "pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''\n",
    "select * \n",
    "from data_ohio_olda_2018.oh_ui_wage_by_quarter\n",
    "limit 5\n",
    "'''\n",
    "pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: __Large tables__ can take a long time to process on shared databases. For example, the Ohio UI wage data has more than 4.7M records per quarter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the number of unique `ssn_hash` values is less than the number of total records in `cc_grads`, indicating that some people have more than one record. Depending on the type of analysis you want to conduct, you need to think about whether you should keep all the records or only keep one record for each person. Here, since you will be exploring graduates' employment outcomes one year later, it would make the most sense to have exactly one graduation record for each person.\n",
    "    \n",
    "So which record should you keep? Again, it depends on the results you want to look at and the assumptions you make about community college students. One may argue that the first degree is more important because it improves a person's educational attainment and make a person more advantaged in the labor market. Others may argue that the most recent degree is more important because it updates a person's skillset and may have a greater impact on their job choices, such as occupation. Here, you will look at a person's most recent record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that `cc_grads` is a table that includes only records of 2012-13 community college graduates, with some `ssn_hash` values appearing multiple times. You will have to do a tiny bit of manuevering to find the most recent graduation record within `cc_grads`. Here, you can assign the first day of the term that corresponds with the correct fiscal quarter (i.e. Summer graduates would graduate on July 1) so you can easily find the most recent graduation date for each `ssn_hash`.\n",
    "> Reminder: This step could be broken into two steps using a temporary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most recent graduation within the span of 2012-13 academic year\n",
    "qry = '''\n",
    "create temp table cc_grads_recent as\n",
    "select distinct on (ssn_hash) *\n",
    "from (\n",
    "SELECT *, \n",
    "    CASE WHEN degcert_term_earned = 4 THEN\n",
    "        format('%%s-%%s-01', degcert_yr_earned, 7)::date \n",
    "    WHEN degcert_term_earned = 1 THEN\n",
    "        format('%%s-%%s-01', degcert_yr_earned, 10)::date \n",
    "    WHEN degcert_term_earned = 2 THEN\n",
    "        format('%%s-%%s-01', degcert_yr_earned, 1)::date \n",
    "    WHEN degcert_term_earned = 3 THEN\n",
    "        format('%%s-%%s-01', degcert_yr_earned, 4)::date \n",
    "    END AS deg_date\n",
    "    from cc_grads\n",
    ") q\n",
    "order by ssn_hash, deg_date DESC\n",
    "'''\n",
    "conn.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_recent_df=pd.read_sql('select * from cc_grads_recent',conn)\n",
    "\n",
    "#Check if we only have one record for each person\n",
    "#Whether the number of records is the same as the number of unique ssn_hash\n",
    "if cc_recent_df['ssn_hash'].count()==cc_recent_df['ssn_hash'].nunique():\n",
    "    print('Each person has one record.')\n",
    "else:\n",
    "    print('Some peole have more than one records.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But because the `oh_ui_wage_by_quarter` table in the `data_ohio_olda_2018` schema is so big, you would run into memory issues if you tried to join `cc_grads_recent` directly with this table. To combat this issue and minimize the time you will have to wait for the following joins to run, you can take a subset of `oh_ui_wage_by_quarter` that will contain employment data within the time frame of the motivating question. The table `small_ohio_ui` table in the `ada_20_osu` schema contains wage data from 2012, 2013 and 2014. We also used the quarter/year combination to create a category `job_date`, which is the first day of the quarter. This will make it easier to join `small_ohio_ui` to `cc_grads_recent`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code was used to create `small_ohio_ui`:\n",
    "\n",
    "    create table ada_20_osu.small_ohio_ui as\n",
    "    select *, format('%%s-%%s-01', year, quarter*3-2)::date job_date \n",
    "    from ada_20_osu.oh_ui_wage_by_quarter\n",
    "    where year in ('2012','2013','2014')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''\n",
    "select * from ada_20_osu.small_ohio_ui limit 5\n",
    "'''\n",
    "pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we want to calculate earnings during the first year after graduation for 2012-13 graduates?**\n",
    "```\n",
    "   Graduation     Earnings during the first year after graduation\n",
    "   \n",
    "    2012_Q3        $2012_Q4+ $2013_Q1+ $2013_Q2+ $2013_Q3\n",
    "   (summer)\n",
    "   \n",
    "    2012_Q4        $2013_Q1+ $2013_Q2+ $2013_Q3+ $2013_Q4\n",
    "   (autumn)\n",
    "   \n",
    "    2013_Q1        $2013_Q2+ $2013_Q3+ $2013_Q4+ $2014_Q1\n",
    "   (winter)\n",
    "   \n",
    "    2013_Q2        $2013_Q3+ $2013_Q4+ $2014_Q1+ $2014_Q2\n",
    "   (spring)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find exactly one year of employment history for every graduate, first you can join `cc_grads_recent` to `small_ohio_ui`, which would have a row for each community college graduate in the 2012-13 academic year that had a job in Ohio in a specific quarter from 2012-14. From there, you can use SQL's understanding of time intervals to subset to jobs for each graduate within one year post-graduation in the `where` clause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, due to the size of the join, you will find yourself waiting around if you run the code yourself. Thus, we've already created the table, which is in the `ada_20_osu` schema and is titled `cohort_oh_jobs`. The code is available for your viewing pleasure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    create table ada_20_osu.cohort_oh_jobs as\n",
    "    select a.ssn_hash, a.deg_date, b.job_date, b.sumwages, (b.job_date - a.deg_date) time_after_grad\n",
    "    from cc_grads_recent a\n",
    "    join ada_20_osu.small_ohio_ui b\n",
    "    on a.ssn_hash = b.ssn_hash\n",
    "    where b.job_date > a.deg_date AND (a.deg_date + '1 year'::interval) >= b.job_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to jobs within 1 year *after* graduation\n",
    "\n",
    "qry = '''\n",
    "select * from ada_20_osu.cohort_oh_jobs\n",
    "'''\n",
    "\n",
    "df_jobs = pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm people don't have more than four quarters worth of earnings\n",
    "# If a person doesn't show in Ohio UI data, we will check his/her employment in Indiana UI data\n",
    "df_jobs.groupby(['ssn_hash']).count()['sumwages'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many people had wages for at least one quarter\n",
    "df_jobs['ssn_hash'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of people who have positive earnings during the first year after graduation\n",
    "df_jobs['ssn_hash'].nunique()/df['ssn_hash'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See distribution of wages per person one year out\n",
    "df_jobs.groupby(['ssn_hash'])['sumwages'].agg('sum').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stable Employment Metric 1**: had positive earnings during ALL four quarters after graduation. \n",
    "```\n",
    "           Quarters after graduation\n",
    "                  \n",
    "           Q1     Q2     Q3      Q4\n",
    "Earning    $X     $X     $X      $X       X>0\n",
    "    \n",
    "```\n",
    "\n",
    "**Stable Employment Metric 2**: worked for the sample employer during the second quarter and the fourth quarter after graduation.\n",
    "\n",
    "```\n",
    "           Quarters after graduation\n",
    "                  \n",
    "           Q1     Q2     Q3      Q4\n",
    "Earning           $X             $X       X>0\n",
    "\n",
    "Employer         1234         1234\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stable employment metric 1:\n",
    "# positive earnings during all four quarters\n",
    "sum(df_jobs.groupby(['ssn_hash']).count()['sumwages'] == 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of people who were employed during all four quarters after graduation\n",
    "sum(df_jobs.groupby(['ssn_hash']).count()['sumwages'] == 4)/df['ssn_hash'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the breakdown of jobs by industry, you need to use the other Ohio wage records table, `oh_ui_wage_by_employer`, since this table contains NAICS codes associated with each employer. You can use a similar process as above to match `cc_grads_recent` to `oh_ui_wage_by_employer`. The table, `cohort_oh_jobs_emp`, has already been created for you using the code below.\n",
    "> The additional clause `employer_num = 1` is used to find the industry of the individual's primary employer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code was used to create `small_ohio_ui_emp`:\n",
    "\n",
    "    create table ada_20_osu.small_ohio_ui_emp as\n",
    "    select *, format('%%s-%%s-01', year, quarter*3-2)::date job_date \n",
    "    from ada_20_osu.oh_ui_wage_by_employer\n",
    "    where year in ('2012','2013','2014') and employer_num = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which was joined to `cc_grads_recent` to create `cohort_oh_jobs_emp`.\n",
    "\n",
    "    create table ada_20_osu.cohort_oh_jobs_emp as\n",
    "    select a.ssn_hash, a.deg_date, b.job_date, b.sumwages, b.naics_3_digit, b.employer, (b.job_date - a.deg_date) time_after_grad\n",
    "    from cc_grads_recent a\n",
    "    join ada_20_osu.small_ohio_ui_emp b\n",
    "    on a.ssn_hash = b.ssn_hash\n",
    "    where b.job_date > a.deg_date AND (a.deg_date + '1 year'::interval) >= b.job_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''\n",
    "select *\n",
    "from ada_20_osu.cohort_oh_jobs_emp\n",
    "'''\n",
    "emp_df = pd.read_sql(qry, conn)\n",
    "\n",
    "emp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the ssn_hash of people who have four quarters of records\n",
    "ssn_4q_df=emp_df.groupby(['ssn_hash'])['wages'].agg(['count']).reset_index()\n",
    "ssn_4q_df=ssn_4q_df[ssn_4q_df['count']==4]\n",
    "\n",
    "#Merge this with emp_df to get industry code\n",
    "emp_4q_df=ssn_4q_df.merge(emp_df,left_on='ssn_hash',right_on='ssn_hash')\n",
    "\n",
    "#Keep the first quarter records only\n",
    "emp_4q_df=emp_4q_df[emp_4q_df['time_after_grad']<=92]\n",
    "emp_4q_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_4q_df.groupby(['naics_3_digit'])['ssn_hash'].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find top 10 industries\n",
    "sort_ind = emp_4q_df.groupby(['naics_3_digit'])['ssn_hash'].count().sort_values(ascending=False)\n",
    "sort_ind.iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lookup table `oh_naics3_codes_lkp` you can join to if you want the corresponding 3-digit NAICS codes in text. We have looked up the top 10 industries with the most graduates who have stable employment:\n",
    "- `622`: Hospital\n",
    "- `722`: Food services and drinking places\n",
    "- `623`: Nursing and residential care facilities\n",
    "- `621`: Ambulatory health care services\n",
    "- `561`: Administrative and support services.\n",
    "- `611`: Education services\n",
    "- `624`: Social assistance\n",
    "- `541`: Professional, scientific, and technical services\n",
    "- `452`: General merchandise stores\n",
    "- `445`: Food and beverage stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, you will analyze stable employment by seeing if a member of this cohort has the same primary employer in the second and fourth quarters post-graduation. You can make use of the `time_after_grad` column you calculated in `cohort_oh_jobs_emp` by finding all jobs that are 2 and 4 quarters after graduation (approximately 182 and 365 days after graduation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are possible values of time_after_grad\n",
    "emp_df['time_after_grad'].sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all jobs 2 and 4 quarters after graduation\n",
    "qry = '''\n",
    "select ssn_hash, deg_date, job_date, employer, naics_3_digit\n",
    "from ada_20_osu.cohort_oh_jobs_emp\n",
    "where time_after_grad between 180 and 185 or time_after_grad = 365\n",
    "'''\n",
    "stable_emp = pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_emp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the amount that had only one employer and showed up in stable_emp twice\n",
    "sum((stable_emp.groupby(['ssn_hash'])['employer'].nunique() == 1) & (stable_emp.groupby(['ssn_hash']).count()['employer'] == 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of people who have stable employment based on metric 2\n",
    "sum((stable_emp.groupby(['ssn_hash'])['employer'].nunique() == 1) & (stable_emp.groupby(['ssn_hash']).count()['employer'] == 2))/df['ssn_hash'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the dataframe of people who worked for the same employer during the 2nd and the 4th quarter after graduation\n",
    "stable_df=stable_emp.groupby(['ssn_hash','employer','naics_3_digit']).count().reset_index()\n",
    "stable_df=stable_df[stable_df['count']==2]\n",
    "\n",
    "#breakdown the number by industry\n",
    "sort_ind2=stable_df.groupby(['naics_3_digit'])['ssn_hash'].count().sort_values(ascending=False)\n",
    "sort_ind2.iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare the number of stable employment defined by the two metrics\n",
    "compare_df=pd.concat([sort_ind.iloc[0:10],sort_ind2.iloc[0:10]],axis=1).reset_index()\n",
    "compare_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint 3: Explore additional earning metrics </h3></font> \n",
    "\n",
    "How many people have positive earnings and earn more than $1,000 during all four quarters after graduation?\n",
    "\n",
    "Hint:\n",
    "1. Get a subsample of table `df_jobs` by restricting `sumwages`.\n",
    "2. Count how many people have 4 records.\n",
    "\n",
    "Afterwards, discuss additional metrics you can use to measure a person's labor market outcomes amongst your group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3>__Motivating Question #3__:</h3></font>\n",
    "**How many 2012-13 Ohio community college graduates found jobs in Indiana one year after graduation? How many of them have stable employment?**\n",
    "\n",
    "In the ADRF, we have education and employment data from other states. This creates us opportunities to examine Ohio graduates' employment across states.\n",
    "\n",
    "In this example, we will focus on the flow of Ohio community college graduates to Indiana. We will use the table `wage_by_employer` in the `in_dwd` schema, which contains both quarterly employment and employer information. Similar to the analysis using Ohio wage records, we've already created smaller versions of the Indiana wage records table `small_indiana_ui` and `small_indiana_ui_emp` in the `ada_20_osu` schema. We've also created permanent tables `cohort_in_wages` and `cohort_in_wages_emp` after joining the most recent record of graduation with the Indiana wage records, whose code is shown below.\n",
    "\n",
    "The analysis will answer the same questions as in Motivating Question #2, while also adding an analysis of cross-state employment flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''\n",
    "select * from in_dwd.wage_by_employer limit 5\n",
    "'''\n",
    "pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code was used to create `small_indiana_ui`:\n",
    "\n",
    "    create table ada_20_osu.small_indiana_ui as\n",
    "    select *, format('%%s-%%s-01', year, quarter*3-2)::date job_date \n",
    "    from in_dwd.wage_by_employer\n",
    "    where year in (2012,2013,2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, the following code was used to create `cohort_in_jobs`:\n",
    "\n",
    "    create table ada_20_osu.cohort_oh_jobs as\n",
    "    select a.ssn_hash, a.deg_date, b.job_date, b.sumwages, (b.job_date - a.deg_date) time_after_grad\n",
    "    from cc_grads_recent a\n",
    "    join ada_20_osu.small_ohio_ui b\n",
    "    on a.ssn_hash = b.ssn_hash\n",
    "    where b.job_date > a.deg_date AND (a.deg_date + '1 year'::interval) >= b.job_date\n",
    "    \n",
    "> Note: An individual may appear more than once per quarter because this table contains all jobs that person worked in the quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see cohort_in_jobs\n",
    "qry = '''\n",
    "select * from ada_20_osu.cohort_in_jobs\n",
    "'''\n",
    "df_in_jobs = pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many people had wages for at least one quarter\n",
    "df_in_jobs['ssn_hash'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stable employment metric 1:\n",
    "# positive earnings during all four quarters\n",
    "# first need to find aggregate earnings per quarter\n",
    "jobs_agg = df_in_jobs.groupby(['ssn_hash', 'job_date'])['wages'].agg('sum').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now can find amount with positive earnings during all four quarters\n",
    "sum(jobs_agg.groupby(['ssn_hash']).count()['wages'] == 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See distribution of wages per person one year out\n",
    "jobs_agg.groupby(['ssn_hash'])['wages'].agg('sum').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the analysis you did to answer Motivating Question #2, you will use `small_indiana_ui_emp` and `cohort_in_wages_emp` to identify those who experienced stable employment, as defined in this notebook. To find the primary employer in Indiana, though, you need to find the employer in each quarter where the individual had the highest earnings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code was used to create `small_indiana_ui_emp`:\n",
    "\n",
    "    create table ada_20_osu.small_indiana_ui_emp as\n",
    "    select distinct on (ssn, year, quarter) *, format('%%s-%%s-01', year, quarter*3-2)::date job_date \n",
    "    from in_dwd.wage_by_employer \n",
    "    where year in (2012, 2013, 2014)\n",
    "    order by ssn, year, quarter, wages desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which was joined to `cc_grads_recent` to create `cohort_in_jobs_emp`.\n",
    "\n",
    "    create table ada_20_osu.cohort_in_jobs_emp as\n",
    "    select a.ssn_hash, a.deg_date, b.job_date, b.wages, b.naics_3_digit, b.fein, (b.job_date - a.deg_date) time_after_grad\n",
    "    from cc_grads_recent a\n",
    "    join ada_20_osu.small_indiana_ui_emp b\n",
    "    on a.ssn_hash = b.ssn\n",
    "    where b.job_date > a.deg_date AND (a.deg_date + '1 year'::interval) >= b.job_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''\n",
    "select * from ada_20_osu.cohort_in_jobs_emp\n",
    "'''\n",
    "emp_df_in = pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find top 10 industries\n",
    "sort_ind = emp_df_in.groupby(['naics_3_digit'])['ssn_hash'].count().sort_values(ascending=False)\n",
    "sort_ind.iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all jobs 2 and 4 quarters after graduation\n",
    "qry = '''\n",
    "select ssn_hash, deg_date, job_date, fein, naics_3_digit\n",
    "from ada_20_osu.cohort_in_jobs_emp\n",
    "where time_after_grad between 180 and 185 or time_after_grad = 365\n",
    "'''\n",
    "stable_emp_in = pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the amount that had only one employer and showed up in stable_emp twice\n",
    "sum((stable_emp_in.groupby(['ssn_hash'])['fein'].nunique() == 1) & (stable_emp_in.groupby(['ssn_hash']).count()['fein'] == 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to analyze cross-state employment patterns, you can union `cohort_in_jobs` with `cohort_oh_jobs`. From there, you can see if a member of your cohort held jobs in both Indiana and Ohio in the same quarter by seeing if they have more than one employer in the quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp table of two job tables unioned\n",
    "qry = '''\n",
    "create temp table jobs_combined as \n",
    "select *, 'in' as state\n",
    "from ada_20_osu.cohort_in_jobs\n",
    "union\n",
    "select *, 'oh' as state \n",
    "from ada_20_osu.cohort_oh_jobs\n",
    "'''\n",
    "conn.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry = '''\n",
    "select * from jobs_combined\n",
    "'''\n",
    "df_combined = pd.read_sql(qry, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to change the factors of time after grad\n",
    "df_combined['time_after_grad'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make even time_after_grad for later\n",
    "df_combined.loc[(df_combined['time_after_grad'] == 91) | (df_combined['time_after_grad'] == 92),'time_after_grad'] = 91\n",
    "df_combined.loc[(df_combined['time_after_grad'] == 182) | (df_combined['time_after_grad'] == 183) | \n",
    "            (df_combined['time_after_grad'] == 184),'time_after_grad'] = 183\n",
    "df_combined.loc[(df_combined['time_after_grad'] == 273) | (df_combined['time_after_grad'] == 274) | \n",
    "            (df_combined['time_after_grad'] == 275),'time_after_grad'] = 273"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.groupby(['ssn_hash', 'time_after_grad'])['wages'].count().unstack(['time_after_grad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_combined.groupby(['ssn_hash', 'time_after_grad'])['wages'].count().unstack(['time_after_grad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN with 0\n",
    "df_tmp.fillna(0, inplace=True)\n",
    "\n",
    "# and set values >1 to 1\n",
    "df_tmp[df_tmp>1] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make ID value a column instead of an index - then we can count it when we group by the 'year_q' columns\n",
    "df_tmp.reset_index(inplace=True)\n",
    "df_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by all columns to count number of people with the same pattern\n",
    "df_tmp.groupby([91, 183, 273, 365])['ssn_hash'].count().reset_index().sort_values(by='ssn_hash', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take this one step further, you could track their employment patterns based on the state they worked in given that they only worked in one state in a given quarter. For this example, though, you can see common employment patterns for this cohort and whether graduates found employment, and in how many states they were employed at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint 4: Explore cross-state employment for 2012-13 Ohio community college graduates</h3></font> \n",
    "\n",
    "Are there any differences between students from Hamilton county colleges and students from non-Hamilton county colleges? Cincinnati metropolitan area is at the intersection between Ohio, Indiana, and Kentucky. Students graduated in this area are easier to find jobs and commute across states. \n",
    "\n",
    "You can find which county an institution is in by using variable `countyname` in table `oh_hei_campus_count_lkp`. What changes do you need to make to the above code?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you have covered how to identify the cohort that you are interested in from a database and save it as dataframe in Python. You have also seen how to conduct descriptive analyses in Python, such as checking missing values and breaking down the sample based on the variables that you are interested.\n",
    "\n",
    "After you find interesting results, you may want to present them in the form of pictures, or visualizations. In the next notebook, which will cover [Data Visualization](02_1_Data_Visualization.ipynb), we will show you how to use `matplotlib` and `seaborn` in Python to display some of your findings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {
    "height": "566px",
    "left": "0px",
    "right": "954px",
    "top": "110px",
    "width": "179px"
   },
   "toc_section_display": "none",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
